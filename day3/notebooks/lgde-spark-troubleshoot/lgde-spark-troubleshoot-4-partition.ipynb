{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3일차 4교시 - Spark Partitioning\n",
    ">  스파크에서 성능을 개선하는 다양한 방법 가운데 파티셔닝과 버킷팅이 있습니다. 이 두가지 기법은 데이터를 저장하는 방법이며, 자주 혹은 함께 읽어지는 데이터 혹은 그룹을 저장 시에 함께 저장해둔다는 개념입니다. 데이터베이스와는 다르게 인덱스를 여러개 가지기 힘든 파일저장 구조의 한계를 극복하기 위해 가장 많이 사용하는 키 혹은 그룹의 정보를 파티셔닝이라는 방식으로 디렉토리를 구분하여 저장할 수 있습니다.\n",
    "\n",
    "### 목차\n",
    "* [1. '파티셔닝' 이란?](#1.-'파티셔닝'-이란?)\n",
    "* [2. '파티셔닝' 잘하는 법?](#2.-'파티셔닝'-잘하는-법?)\n",
    "* [3. 파티션 다루기](#3.-파티션-다루기)\n",
    "* [4. 파티션의 특징](#4.-파티션의-특징)\n",
    "\n",
    "### 참고자료\n",
    "  * https://techmagie.wordpress.com/2015/12/19/understanding-spark-partitioning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/04 08:48:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c7f7ce8f1708:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6ac5bc7df0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "\n",
    "# 공통 데이터 위치\n",
    "home_jovyan = \"/home/jovyan\"\n",
    "work_data = f\"{home_jovyan}/work/data\"\n",
    "work_dir=!pwd\n",
    "work_dir = work_dir[0]\n",
    "\n",
    "# 로컬 환경 최적화\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) # the number of partitions to use when shuffling data for joins or aggregations.\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. '파티셔닝' 이란?\n",
    "> 분산환경에서 큰 데이터를 특정 노드에 모두 저장할 수 없기 때문에 여러 노드에 분산하여 저장하는데 이를 파티션이라고 하며, 스파크에서는 자동으로 RDD 를 파티셔닝 하고 분산 저장합니다. 스파크 내에서 파티션은 데이터 저장의 단위이자 병렬수행의 기본단위입니다.\n",
    "\n",
    "#### 1.1 파티셔닝의 특징\n",
    "* 하나의 익스큐터 당 하나의 파티션을 할당하므로 파티션 수가 전체 성능에 큰 영향을 미침 (ex_ sc.textFile(\"files\", 5))\n",
    "* 너무 파티션 수가 작은 경우\n",
    "  * 병렬 수행의 장점을 누리기 어렵습니다\n",
    "  * 데이터 편중에 따른 리소스 활용이 떨어질 수 있습니다\n",
    "* 너무 파티션 수가 많은 경우\n",
    "  * 실제 수행에 필요한 시간보다 작업 수행에 걸리는 시간이 더 오래 걸릴 수 있습니다\n",
    "* 파티션 수와 성능은 항상 트레이드 오프 관계에 있으므로 밸런싱이 중요합니다\n",
    "  * Average : 클러스터의 크기에 따라 다르지만 100 ~ 10K 수준의 파티션 수가 적절합니다\n",
    "  * Lower Bound : 어플리케이션에 할당된 대상 클러스터 전체의 코어 수의 2배 정도입니다\n",
    "  * Upper Bound : 반대로 타스크 수행에 100ms 가 안 걸린다면 데이터가 너무 작아 스케줄링에 더 많은 리소스가 소모되고 있습니다\n",
    "\n",
    "#### 1.2. 파티셔닝의 종류\n",
    "* None Partitioning → 파티셔닝이 데이터의 특성에 기반하지는 않지만, 모든 노드에 고르게 랜덤하게 분포됨을 말합니다\n",
    "  * 데이터 Reading, Parallelize 등의 경우에는 None \n",
    "* Hash Partitioning → Object.hashCode 값을 기준으로 균등하게 노드에 배포하며 hashCode % numPartitions 로 결정합니다\n",
    "  * reduceBy, groupBy, join 등과 같은 shuffle 이 발생하는 경우는 Hash\n",
    "* Range Partitioning → RDD 의 키 가운데 순서가 의미있는 경우 가까운 범위의 값들을 정렬된 키를 기준으로 뭉쳐둘 수 있습니다\n",
    "  * sortBy 와 같은 경우는 Range 입니다.\n",
    "\n",
    "#### 1.3. 파티션은 언제 생성되는가?\n",
    "* dataframe.partitionBy 과 같이 명시적으로 파티셔닝을 수행하거나\n",
    "* shuffle 이 발생하는 연산(join, groupBy, reduceBy, foldBy, sort, partitionBy 연산) 수행 시\n",
    "\n",
    "#### 1.4. 파티션 구성요소 확인\n",
    "* glom() → 하나의 파티션에 존재하는 모든 요소 정보들을 모아 하나의 RDD 로 반환하는 API\n",
    "* partitioner → RDD 로부터 partitioner 정보를 조사합니다\n",
    "* 다양한 집계 및 조인 함수 수행 시에 셔플링이 발생하며 이 때에 파티셔닝 설계가 중요합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 6\n",
      "Partitioner: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions structure: [[], [1], [2], [3], [4], [5]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "data = [1, 2, 3, 4, 5]\n",
    "\n",
    "rdd = sc.parallelize(data)\n",
    "print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "print(\"Partitions structure: {}\".format(rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2. '파티셔닝' 잘 하는 법\n",
    "> 셔플이 발생하는 다양한 변환작업 중에 skew 가 발생한 특정 노드에 셔플 블락이 2GB 이상 집중되는 경우, 자바의 바이트 버퍼 제한으로 실행에 실패합니다\n",
    "\n",
    "* 내가 사용하는 데이터소스의 크기, 타입과 데이터의 분포를 파악\n",
    "  * 데이터소스의 split 가능여부, 편중되어 저장되어 있는지 혹은 파티셔닝이 균질하게 잘 저장되도록 전처리가 필요할 수도 있습니다\n",
    "* 데이터 소스에 대해 어떠한 연산자를 사용하는 것이 적절한 지 파악\n",
    "  * 어떤 경우에 어떤 집계 연산을 사용하면 좋은지 팥단이 필요함 - reduceByKey, aggregateByKey\n",
    "* 임의의 데이터 소스 혹은 셔플 키에 대한 파티셔닝은 솔팅 기법을 적용하는 것을 검토\n",
    "  * 편중이 심한 파티션 그룹에 대해 상대적으로 큰 파티션 조인 파트에 뻥튀기가 되도록 explode 통한 salting key 부여\n",
    "  * 나머지 파티션 조인 파트에는 동일한 키 그룹에 대해 salting key 부여 하여 해당 키 가지수 만큼 병렬성 부여"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 3. 파티션 다루기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight = spark.read.parquet(f\"{work_data}/flight-data/parquet/2010-summary.parquet\")\n",
    "flight.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "|    United States|          Singapore|   25|\n",
      "|    United States|            Grenada|   54|\n",
      "|       Costa Rica|      United States|  477|\n",
      "|          Senegal|      United States|   29|\n",
      "|    United States|   Marshall Islands|   44|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "flight.write.mode(\"overwrite\").partitionBy(\"DEST_COUNTRY_NAME\").parquet(\"target/troubleshoot4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      "\n",
      "+--------------------+-----+-----------------+\n",
      "| ORIGIN_COUNTRY_NAME|count|DEST_COUNTRY_NAME|\n",
      "+--------------------+-----+-----------------+\n",
      "|             Romania|    1|    United States|\n",
      "|             Ireland|  264|    United States|\n",
      "|               India|   69|    United States|\n",
      "|           Singapore|   25|    United States|\n",
      "|             Grenada|   54|    United States|\n",
      "|    Marshall Islands|   44|    United States|\n",
      "|        Sint Maarten|   53|    United States|\n",
      "|         Afghanistan|    2|    United States|\n",
      "|              Russia|  156|    United States|\n",
      "|Federated States ...|   48|    United States|\n",
      "+--------------------+-----+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "united_states = spark.read.parquet(\"target/troubleshoot4\").where(\"DEST_COUNTRY_NAME='United States'\")\n",
    "united_states.printSchema()\n",
    "united_states.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "flight.write.mode(\"overwrite\").partitionBy(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").parquet(\"target/troubleshoot4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- count: long (nullable = true)\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      "\n",
      "+-----+-----------------+-------------------+\n",
      "|count|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----+-----------------+-------------------+\n",
      "|  621|    United States|        South Korea|\n",
      "+-----+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "us_sk = spark.read.parquet(\"target/troubleshoot4\").where(\"DEST_COUNTRY_NAME='United States' and ORIGIN_COUNTRY_NAME='South Korea'\")\n",
    "us_sk.printSchema()\n",
    "us_sk.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 4. 파티셔닝 주의할 점\n",
    "> 파티션 구성 시에 데이터의 크기에 따른 파티션 선정이 중요한데 너무 작은 데이터 파티션에서 너무 다양한 파티션 값을 가진 경우 오히려 성능이 떨어지는 경우도 발생할 수 있습니다.\n",
    "\n",
    "* 파티션의 수가 너무 많은 경우 성능이 떨어지는 예제\n",
    "* 파티션의 수에 따라 파일 I/O가 달라지는 예제\n",
    "\n",
    "> 로컬 환경에서 수백MB 데이터 예제 수행 시에 정상적으로 동작하지 않기 때문에 성능이 더 좋아지는 예제를 포함하지 못한 점 양해 부탁 드립니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = spark.read.option(\"inferSchema\", True).json(f\"{work_data}/flight-data/json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal.write.mode(\"overwrite\").parquet(\"target/part_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "normal.write.mode(\"overwrite\").partitionBy(\"DEST_COUNTRY_NAME\").parquet(\"target/part_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[ORIGIN_COUNTRY_NAME#368], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(ORIGIN_COUNTRY_NAME#368, 5), ENSURE_REQUIREMENTS, [id=#432]\n",
      "   +- *(1) HashAggregate(keys=[ORIGIN_COUNTRY_NAME#368], functions=[partial_count(1)])\n",
      "      +- *(1) Project [ORIGIN_COUNTRY_NAME#368]\n",
      "         +- *(1) Filter (isnotnull(DEST_COUNTRY_NAME#367) AND (DEST_COUNTRY_NAME#367 = United States))\n",
      "            +- *(1) ColumnarToRow\n",
      "               +- FileScan parquet [DEST_COUNTRY_NAME#367,ORIGIN_COUNTRY_NAME#368] Batched: true, DataFilters: [isnotnull(DEST_COUNTRY_NAME#367), (DEST_COUNTRY_NAME#367 = United States)], Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/lgde-spark-troubleshoot/target/part_v1], PartitionFilters: [], PushedFilters: [IsNotNull(DEST_COUNTRY_NAME), EqualTo(DEST_COUNTRY_NAME,United States)], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string>\n",
      "\n",
      "\n",
      "+-------------------+-----+\n",
      "|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------------+-----+\n",
      "|Ireland            |6    |\n",
      "+-------------------+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "CPU times: user 4.19 ms, sys: 1.15 ms, total: 5.34 ms\n",
      "Wall time: 294 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "normal = spark.read.parquet(\"target/part_v1\")\n",
    "normal_expr = normal.where(expr(\"DEST_COUNTRY_NAME = 'United States'\")).groupBy(\"ORIGIN_COUNTRY_NAME\").count()\n",
    "normal_expr.explain()\n",
    "normal_expr.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[ORIGIN_COUNTRY_NAME#391], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(ORIGIN_COUNTRY_NAME#391, 5), ENSURE_REQUIREMENTS, [id=#498]\n",
      "   +- *(1) HashAggregate(keys=[ORIGIN_COUNTRY_NAME#391], functions=[partial_count(1)])\n",
      "      +- *(1) Project [ORIGIN_COUNTRY_NAME#391]\n",
      "         +- *(1) ColumnarToRow\n",
      "            +- FileScan parquet [ORIGIN_COUNTRY_NAME#391,DEST_COUNTRY_NAME#393] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/lgde-spark-troubleshoot/target/part_v2], PartitionFilters: [isnotnull(DEST_COUNTRY_NAME#393), (DEST_COUNTRY_NAME#393 = United States)], PushedFilters: [], ReadSchema: struct<ORIGIN_COUNTRY_NAME:string>\n",
      "\n",
      "\n",
      "+-------------------+-----+\n",
      "|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------------+-----+\n",
      "|Ireland            |6    |\n",
      "+-------------------+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "CPU times: user 5.92 ms, sys: 4.35 ms, total: 10.3 ms\n",
      "Wall time: 1.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "partitioned = spark.read.parquet(\"target/part_v2\")\n",
    "partitioned_expr = partitioned.where(expr(\"DEST_COUNTRY_NAME = 'United States'\")).groupBy(\"ORIGIN_COUNTRY_NAME\").count()\n",
    "partitioned_expr.explain()\n",
    "partitioned_expr.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Arrival_Time: long (nullable = true)\n",
      " |-- Creation_Time: long (nullable = true)\n",
      " |-- Device: string (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- gt: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n",
      "+-------------+-------------------+--------+-----+------+----+-----+------------+-----------+-----------+\n",
      "|Arrival_Time |Creation_Time      |Device  |Index|Model |User|gt   |x           |y          |z          |\n",
      "+-------------+-------------------+--------+-----+------+----+-----+------------+-----------+-----------+\n",
      "|1424686734964|1424688581018556031|nexus4_2|1    |nexus4|g   |stand|-0.001449585|0.035491943|0.027999878|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+------------+-----------+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "activity = spark.read.option(\"inferSchema\", True).json(f\"{work_data}/activity-data\")\n",
    "activity.printSchema()\n",
    "activity.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "activity.write.mode(\"overwrite\").parquet(\"target/normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "activity.write.partitionBy(\"User\").mode(\"overwrite\").parquet(\"target/partition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 ms, sys: 2.68 ms, total: 4.69 ms\n",
      "Wall time: 120 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>gt</th><th>count</th></tr>\n",
       "<tr><td>sit</td><td>8364</td></tr>\n",
       "<tr><td>stand</td><td>8058</td></tr>\n",
       "<tr><td>stairsdown</td><td>5995</td></tr>\n",
       "<tr><td>walk</td><td>9480</td></tr>\n",
       "<tr><td>null</td><td>10476</td></tr>\n",
       "<tr><td>stairsup</td><td>6557</td></tr>\n",
       "<tr><td>bike</td><td>8688</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+-----+\n",
       "|        gt|count|\n",
       "+----------+-----+\n",
       "|       sit| 8364|\n",
       "|     stand| 8058|\n",
       "|stairsdown| 5995|\n",
       "|      walk| 9480|\n",
       "|      null|10476|\n",
       "|  stairsup| 6557|\n",
       "|      bike| 8688|\n",
       "+----------+-----+"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "spark.read.parquet(\"target/normal\").where(\"User = 'e'\").groupBy(\"gt\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.32 ms, sys: 1.35 ms, total: 3.68 ms\n",
      "Wall time: 266 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>gt</th><th>count</th></tr>\n",
       "<tr><td>sit</td><td>8364</td></tr>\n",
       "<tr><td>stand</td><td>8058</td></tr>\n",
       "<tr><td>stairsdown</td><td>5995</td></tr>\n",
       "<tr><td>walk</td><td>9480</td></tr>\n",
       "<tr><td>null</td><td>10476</td></tr>\n",
       "<tr><td>stairsup</td><td>6557</td></tr>\n",
       "<tr><td>bike</td><td>8688</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+-----+\n",
       "|        gt|count|\n",
       "+----------+-----+\n",
       "|       sit| 8364|\n",
       "|     stand| 8058|\n",
       "|stairsdown| 5995|\n",
       "|      walk| 9480|\n",
       "|      null|10476|\n",
       "|  stairsup| 6557|\n",
       "|      bike| 8688|\n",
       "+----------+-----+"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "spark.read.parquet(\"target/partition\").where(\"User = 'e'\").groupBy(\"gt\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.4 ms, sys: 1.85 ms, total: 4.25 ms\n",
      "Wall time: 13.2 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>gt</th><th>count</th></tr>\n",
       "<tr><td>sit</td><td>4182</td></tr>\n",
       "<tr><td>stand</td><td>4029</td></tr>\n",
       "<tr><td>stairsdown</td><td>2997</td></tr>\n",
       "<tr><td>walk</td><td>4740</td></tr>\n",
       "<tr><td>null</td><td>5238</td></tr>\n",
       "<tr><td>stairsup</td><td>3279</td></tr>\n",
       "<tr><td>bike</td><td>4344</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+-----+\n",
       "|        gt|count|\n",
       "+----------+-----+\n",
       "|       sit| 4182|\n",
       "|     stand| 4029|\n",
       "|stairsdown| 2997|\n",
       "|      walk| 4740|\n",
       "|      null| 5238|\n",
       "|  stairsup| 3279|\n",
       "|      bike| 4344|\n",
       "+----------+-----+"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "activity.unpersist()\n",
    "activity.where(\"User = 'e'\").groupBy(\"gt\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "### 5. 파티션의 특징\n",
    "\n",
    "* Number of Tasks on per stage basis = Number of partitions\n",
    "  * 특정 스테이지의 타스크 수는 파티션 수와 같습니다\n",
    "  * 즉, \n",
    "* 같은 파티션에 존재하는 튜플들은 반드시 같은 장비에 존재한다\n",
    "* 하나의 파티션에는 하나의 타스크만 할당되며, 워커는 한 번에 하나의 타스크만 수행합니다\n",
    "* 스파크 셔플 블럭들의 크기는 최대 2GB를 넘지 못합니다\n",
    "  * 자바 바이트버퍼 추상화 객체의 MAX_SIZE 가 2GB 이기 때문입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
