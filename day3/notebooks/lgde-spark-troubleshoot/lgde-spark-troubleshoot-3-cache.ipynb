{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3일차 3교시 - Spark Cache, Persist and Unpersist\n",
    ">  데이터 캐싱 즉 메모리에 저장하여 성능을 향상 시키는 방식에 대해 이해하고 적용합니다. 기본적으로  스파크는 필요한 데이터 소스의 경우 리니지를 통해 다시 계산하는 것을 원칙으로 하므로 자주 사용되는 중간 데이터의 경우 메모리에 저장해둔다면 유용할 수 있습니다. 일반 ETL 작업 보다는 그래프 연산이나 기계학습 등에 반복적인 처리과정에 효과적입니다.\n",
    "\n",
    "## 개요\n",
    "> 스파크에는 메모리를 기반으로 데이터를 처리하는 것을 기본 전략으로 체택하고 있어 메모리만 충분하다면 자주 사용하는 테이블(데이터 프레임)을 캐싱해 둘 수 있는데 이를 통해 자주 조인되거나 잘 변경되지 않는 데이터를 캐싱하는 기법을 학습합니다\n",
    "\n",
    "* 다양한 캐싱 기법\n",
    "* 캐싱 전략과 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/04 08:27:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c7f7ce8f1708:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcad0688df0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "\n",
    "# 공통 데이터 위치\n",
    "home_jovyan = \"/home/jovyan\"\n",
    "work_data = f\"{home_jovyan}/work/data\"\n",
    "work_dir=!pwd\n",
    "work_dir = work_dir[0]\n",
    "\n",
    "# 로컬 환경 최적화\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) # the number of partitions to use when shuffling data for joins or aggregations.\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## I. 다양한 캐싱 기법\n",
    "> `Spark SQL` 혹은 `Structured API` 수준에서 캐싱 명령어는 각각 다르며 조금씩 다르게 동작합니다\n",
    "\n",
    "\n",
    "### 1. SparkSQL 통한 캐싱\n",
    "\n",
    "```python\n",
    "# RDD 자체를 캐싱하는 cache 혹은 persist 연산과는 다르게 즉시 테이블을 캐싱합니다.\n",
    "spark.sql(\"CACHE TABLE [tableName]\")\n",
    "# 물론 LAZY 키워드를 이용하면 지연된 캐싱도 가능합니다\n",
    "spark.sql(\"CACHE LAZY TABLE [tableName]\")\n",
    "# 캐시 테이블을 다시 리프래시 할 수 있습니다\n",
    "spark.sql(\"REFRESH TABLE [tableName]\")\n",
    "# // 캐시로부터 해당 테이블을 제거합니다\n",
    "spark.sql(\"UNCACHE TABLE IF EXISTS [tableName]\")\n",
    "# // 캐시로부터 모든 테이블들을 제거합니다\n",
    "spark.sql(\"CLEAR CACHE\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "t1 = spark.range(1, 1000)\n",
    "t1.createOrReplaceTempView(\"t1\")\n",
    "spark.sql(\"cache table t1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Structured API 통한 캐싱\n",
    "\n",
    "#### 2.1. Cache 또한 Transformation 입니다.\n",
    "> 즉, Lazy Evaluation 으로 동작하며, 그 즉시 캐싱되지 않습니다. (dataframe.cache() != cache table)\n",
    "Segment of Logical Plan → CacheManager → Cached Data 순서대로 동작합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* 스칼라를 통한 캐시 적용 및 플랜을 확인하는 방법 (파이썬에서는 동작하지 않습니다)\n",
    "```scala\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder.appName(\"scala-cache\").getOrCreate()\n",
    "val data = spark.range(1).cache()\n",
    "println(data.queryExecution.withCachedData.numberedTreeString)\n",
    "```\n",
    "<br>\n",
    "\n",
    "* 출력된 내용은 다음과 같습니다.\n",
    "```bash\n",
    "00 InMemoryRelation [id#0L], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
    "01    +- *(1) Range (0, 1, step=1, splits=8)\n",
    "```\n",
    "<br>\n",
    "\n",
    "* 변환의 플랜을 확인하고자 한다면\n",
    "```scala\n",
    "data.withColumn(\"newId\", 'id).explain(extended = true)\n",
    "```\n",
    "<br>\n",
    "\n",
    "* 플랜 내역을 확인해봅니다.\n",
    "```text\n",
    "== Parsed Logical Plan ==\n",
    "  'Project [id#32L, 'id AS newId#34]\n",
    "    +- Range (0, 1, step=1, splits=Some(8))\n",
    "== Analyzed Logical Plan ==\n",
    "id: bigint, newId: bigint\n",
    "  Project [id#32L, id#32L AS newId#34L]\n",
    "    +- Range (0, 1, step=1, splits=Some(8))\n",
    "== Optimized Logical Plan ==\n",
    "  Project [id#32L, id#32L AS newId#34L]\n",
    "    +- InMemoryRelation [id#32L], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
    "      +- *(1) Range (0, 1, step=1, splits=8)\n",
    "== Physical Plan ==\n",
    "  *(1) Project [id#32L, id#32L AS newId#34L]\n",
    "    +- *(1) InMemoryTableScan [id#32L]\n",
    "      +- InMemoryRelation [id#32L], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
    "        +- *(1) Range (0, 1, step=1, splits=8)\n",
    "```\n",
    "<br>\n",
    "\n",
    "* 현재 캐시 여부를 확인하고자 한다면\n",
    "```scala\n",
    "val cache = spark.sharedState.cacheManager\n",
    "cache.lookupCachedData(data.queryExecution.logical).isDefined\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = spark.read.parquet(\"source/s1\")\n",
    "s2 = spark.read.parquet(\"source/s2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- registration: string (nullable = true)\n",
      " |-- make: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- engine_size: decimal(38,18) (nullable = true)\n",
      "\n",
      "root\n",
      " |-- make: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- engine_size: decimal(38,18) (nullable = true)\n",
      " |-- sale_price: double (nullable = true)\n",
      "\n",
      "+------------+----+------+--------------------+\n",
      "|registration|make| model|         engine_size|\n",
      "+------------+----+------+--------------------+\n",
      "|     CPbYgbw|FORD|FIESTA|1.300000000000000000|\n",
      "+------------+----+------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----+-----+--------------------+----------+\n",
      "|make|model|         engine_size|sale_price|\n",
      "+----+-----+--------------------+----------+\n",
      "|FIAT|  500|1.100000000000000000|    1610.0|\n",
      "+----+-----+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s1.printSchema()\n",
    "s2.printSchema()\n",
    "s1.show(1)\n",
    "s2.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## II. 캐싱 전략과 활용\n",
    "> 캐싱을 사용할 때에는 `Cache` 와 `Persist`의 차이점을 잘 이해하고 사용해야 합니다.\n",
    "\n",
    "### 3. Cache vs. Persist 비교\n",
    "\n",
    "#### 3.1 Cache \n",
    "* RDD cache() 메소드는 기본이 MEMORY_ONLY 캐싱이며 \n",
    "* DataFrame, DataSet cache() 메소드는 기본이 MEMORY_AND_DISK 캐싱입니다\n",
    "* cache 메소드는 결국 내부적으로 persist 함수를 호출하며 sparkSession.sharedState.cacheManger.cacherQuery 에서 관리됩니다.\n",
    "\n",
    "#### 3.2 Persist\n",
    "* persist() 메소드는 STORAGE 레벨을 직접 지정할 수 있습니다\n",
    "* DataFrame 혹은 DataSet 의 cache() 함수는 persist(StorageLevel.MEMORY_AND_DISK)와 동일합니다\n",
    "\n",
    "#### 3.3 Unpersist\n",
    "* 캐시된 데이터를 제거하며 모든 블록이 제거될 때 까지 블록되는 옵션을 가집니다 unpersist(default=true)\n",
    "```python\n",
    "cachedData = df.cache()\n",
    "cachedData.show()\n",
    "cachedData.unpersist()  # 당연하겠지만 df 또한 unpersist 됩니다\n",
    "```\n",
    "\n",
    "#### 3.4 데이터프레임을 통한 캐싱 여부 확인\n",
    "* c12 StorageLevel(disk, memory, deserialized, 1 replicas)\n",
    "* StorageLevel(memory, 1 replicas)\n",
    "* 참고로 spark 은 replication 을 직접 구현하지 않으며 hdfs 의 것을 그대로 활용합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [make#37, model#38], [make#44, model#45], Inner, BuildLeft, false\n",
      ":- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, false], input[2, string, false]),false), [id=#104]\n",
      ":  +- *(1) Filter (isnotnull(make#37) AND isnotnull(model#38))\n",
      ":     +- *(1) ColumnarToRow\n",
      ":        +- FileScan parquet [registration#36,make#37,model#38,engine_size#39] Batched: true, DataFilters: [isnotnull(make#37), isnotnull(model#38)], Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/lgde-spark-troubleshoot/source/s1], PartitionFilters: [], PushedFilters: [IsNotNull(make), IsNotNull(model)], ReadSchema: struct<registration:string,make:string,model:string,engine_size:decimal(38,18)>\n",
      "+- *(2) Filter (((isnotnull(sale_price#47) AND (sale_price#47 > 2000.0)) AND isnotnull(make#44)) AND isnotnull(model#45))\n",
      "   +- *(2) ColumnarToRow\n",
      "      +- FileScan parquet [make#44,model#45,engine_size#46,sale_price#47] Batched: true, DataFilters: [isnotnull(sale_price#47), (sale_price#47 > 2000.0), isnotnull(make#44), isnotnull(model#45)], Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/lgde-spark-troubleshoot/source/s2], PartitionFilters: [], PushedFilters: [IsNotNull(sale_price), GreaterThan(sale_price,2000.0), IsNotNull(make), IsNotNull(model)], ReadSchema: struct<make:string,model:string,engine_size:decimal(38,18),sale_price:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "c12 = s1.join(s2, [s1.make == s2.make, s1.model == s2.model])\n",
    "unpersist = c12.where(col(\"sale_price\") > 2000.0).unpersist()\n",
    "unpersist.explain()  # Default StorageLevel(disk, memory, deserialized, 1 replicas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [registration#36, make#37, model#38, engine_size#39, make#44, model#45, engine_size#46, sale_price#47]\n",
      "   +- InMemoryRelation [registration#36, make#37, model#38, engine_size#39, make#44, model#45, engine_size#46, sale_price#47], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(2) BroadcastHashJoin [make#37, model#38], [make#44, model#45], Inner, BuildLeft, false\n",
      "            :- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, false], input[2, string, false]),false), [id=#143]\n",
      "            :  +- *(1) Filter (isnotnull(make#37) AND isnotnull(model#38))\n",
      "            :     +- *(1) ColumnarToRow\n",
      "            :        +- FileScan parquet [registration#36,make#37,model#38,engine_size#39] Batched: true, DataFilters: [isnotnull(make#37), isnotnull(model#38)], Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/lgde-spark-troubleshoot/source/s1], PartitionFilters: [], PushedFilters: [IsNotNull(make), IsNotNull(model)], ReadSchema: struct<registration:string,make:string,model:string,engine_size:decimal(38,18)>\n",
      "            +- *(2) Filter (isnotnull(make#44) AND isnotnull(model#45))\n",
      "               +- *(2) ColumnarToRow\n",
      "                  +- FileScan parquet [make#44,model#45,engine_size#46,sale_price#47] Batched: true, DataFilters: [isnotnull(make#44), isnotnull(model#45)], Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/lgde-spark-troubleshoot/source/s2], PartitionFilters: [], PushedFilters: [IsNotNull(make), IsNotNull(model)], ReadSchema: struct<make:string,model:string,engine_size:decimal(38,18),sale_price:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cachedDiskMemory = c12.cache()\n",
    "cachedDiskMemory.explain()  # StorageLevel(disk, memory, deserialized, 1 replicas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [registration#36, make#37, model#38, engine_size#39, make#44, model#45, engine_size#46, sale_price#47]\n",
      "   +- InMemoryRelation [registration#36, make#37, model#38, engine_size#39, make#44, model#45, engine_size#46, sale_price#47], StorageLevel(memory, 1 replicas)\n",
      "         +- *(1) Filter (isnotnull(sale_price#47) AND (sale_price#47 > 2100.0))\n",
      "            +- InMemoryTableScan [registration#36, make#37, model#38, engine_size#39, make#44, model#45, engine_size#46, sale_price#47], [isnotnull(sale_price#47), (sale_price#47 > 2100.0)]\n",
      "                  +- InMemoryRelation [registration#36, make#37, model#38, engine_size#39, make#44, model#45, engine_size#46, sale_price#47], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                        +- *(2) BroadcastHashJoin [make#37, model#38], [make#44, model#45], Inner, BuildLeft, false\n",
      "                           :- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, false], input[2, string, false]),false), [id=#143]\n",
      "                           :  +- *(1) Filter (isnotnull(make#37) AND isnotnull(model#38))\n",
      "                           :     +- *(1) ColumnarToRow\n",
      "                           :        +- FileScan parquet [registration#36,make#37,model#38,engine_size#39] Batched: true, DataFilters: [isnotnull(make#37), isnotnull(model#38)], Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/lgde-spark-troubleshoot/source/s1], PartitionFilters: [], PushedFilters: [IsNotNull(make), IsNotNull(model)], ReadSchema: struct<registration:string,make:string,model:string,engine_size:decimal(38,18)>\n",
      "                           +- *(2) Filter (isnotnull(make#44) AND isnotnull(model#45))\n",
      "                              +- *(2) ColumnarToRow\n",
      "                                 +- FileScan parquet [make#44,model#45,engine_size#46,sale_price#47] Batched: true, DataFilters: [isnotnull(make#44), isnotnull(model#45)], Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/lgde-spark-troubleshoot/source/s2], PartitionFilters: [], PushedFilters: [IsNotNull(make), IsNotNull(model)], ReadSchema: struct<make:string,model:string,engine_size:decimal(38,18),sale_price:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cachedMemoryOnly = c12.where(col(\"sale_price\") > 2100.0).persist(StorageLevel.MEMORY_ONLY)\n",
    "cachedMemoryOnly.explain()  # StorageLevel(memory, 1 replicas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 4. Cached 여부에 따른 성능 차이 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>registration</th><th>make</th><th>model</th><th>engine_size</th><th>make</th><th>model</th><th>engine_size</th><th>sale_price</th></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>1610.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.200000000000000000</td><td>3283.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.200000000000000000</td><td>5371.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.000000000000000000</td><td>4341.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.000000000000000000</td><td>2679.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.200000000000000000</td><td>4836.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.000000000000000000</td><td>1482.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.000000000000000000</td><td>1144.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>1162.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.200000000000000000</td><td>2325.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.000000000000000000</td><td>4271.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.200000000000000000</td><td>665.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>2973.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.200000000000000000</td><td>758.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.200000000000000000</td><td>4230.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>1924.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.000000000000000000</td><td>3456.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.000000000000000000</td><td>1647.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.000000000000000000</td><td>3074.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>4855.0</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+------------+----+-----+--------------------+----+-----+--------------------+----------+\n",
       "|registration|make|model|         engine_size|make|model|         engine_size|sale_price|\n",
       "+------------+----+-----+--------------------+----+-----+--------------------+----------+\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.100000000000000000|    1610.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.200000000000000000|    3283.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.200000000000000000|    5371.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.000000000000000000|    4341.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.000000000000000000|    2679.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.200000000000000000|    4836.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.000000000000000000|    1482.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.000000000000000000|    1144.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.100000000000000000|    1162.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.200000000000000000|    2325.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.000000000000000000|    4271.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.200000000000000000|     665.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.100000000000000000|    2973.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.200000000000000000|     758.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.200000000000000000|    4230.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.100000000000000000|    1924.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.000000000000000000|    3456.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.000000000000000000|    1647.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.000000000000000000|    3074.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.100000000000000000|    4855.0|\n",
       "+------------+----+-----+--------------------+----+-----+--------------------+----------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "s1 = spark.read.parquet(\"source/s1\").limit(1000)\n",
    "s2 = spark.read.parquet(\"source/s2\").limit(10000)\n",
    "cond = [s1.make == s2.make, s1.model == s2.model]\n",
    "sale_price = s1.join(s2, cond).filter(s2.engine_size - s1.engine_size <= 0.1)\n",
    "sale_price.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|registration|price             |\n",
      "+------------+------------------+\n",
      "|TxRX6kA     |3238.9210526315787|\n",
      "|PX4Aq0t     |3238.9210526315787|\n",
      "|ALS3XKU     |3238.9210526315787|\n",
      "|JBlTWUg     |3238.9210526315787|\n",
      "|b03GIjW     |3238.9210526315787|\n",
      "|atj0cou     |3163.0254237288136|\n",
      "|eFTv6hF     |3163.0254237288136|\n",
      "|Rz1GRum     |3163.0254237288136|\n",
      "|UPDkPxK     |3163.0254237288136|\n",
      "|zmpYII0     |3163.0254237288136|\n",
      "+------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 9.63 ms, sys: 2.19 ms, total: 11.8 ms\n",
      "Wall time: 1.79 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# cache 하지 않은 상태에서 Join 할 때 \n",
    "sale_price.groupBy(\"registration\").agg(avg(\"sale_price\").alias(\"price\")).orderBy(desc(\"price\")).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>registration</th><th>make</th><th>model</th><th>engine_size</th><th>make</th><th>model</th><th>engine_size</th><th>sale_price</th></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>1610.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.200000000000000000</td><td>3283.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.200000000000000000</td><td>5371.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.000000000000000000</td><td>4341.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.000000000000000000</td><td>2679.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.200000000000000000</td><td>4836.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.000000000000000000</td><td>1482.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.000000000000000000</td><td>1144.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>1162.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.200000000000000000</td><td>2325.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.000000000000000000</td><td>4271.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.200000000000000000</td><td>665.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>2973.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.200000000000000000</td><td>758.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.200000000000000000</td><td>4230.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>1924.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.000000000000000000</td><td>3456.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.000000000000000000</td><td>1647.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.000000000000000000</td><td>3074.0</td></tr>\n",
       "<tr><td>mK5LtWT</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>FIAT</td><td>500</td><td>1.100000000000000000</td><td>4855.0</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+------------+----+-----+--------------------+----+-----+--------------------+----------+\n",
       "|registration|make|model|         engine_size|make|model|         engine_size|sale_price|\n",
       "+------------+----+-----+--------------------+----+-----+--------------------+----------+\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.100000000000000000|    1610.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.200000000000000000|    3283.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.200000000000000000|    5371.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.000000000000000000|    4341.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.000000000000000000|    2679.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.200000000000000000|    4836.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.000000000000000000|    1482.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.000000000000000000|    1144.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.100000000000000000|    1162.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.200000000000000000|    2325.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.000000000000000000|    4271.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.200000000000000000|     665.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.100000000000000000|    2973.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.200000000000000000|     758.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.200000000000000000|    4230.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.100000000000000000|    1924.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.000000000000000000|    3456.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.000000000000000000|    1647.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.000000000000000000|    3074.0|\n",
       "|     mK5LtWT|FIAT|  500|1.100000000000000000|FIAT|  500|1.100000000000000000|    4855.0|\n",
       "+------------+----+-----+--------------------+----+-----+--------------------+----------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "s3 = spark.read.parquet(\"source/s1\").limit(1000)\n",
    "s4 = spark.read.parquet(\"source/s2\").limit(10000)\n",
    "cond = [s3.make == s4.make, s3.model == s4.model]\n",
    "cached_price = s3.join(s4, cond).filter(s4.engine_size - s3.engine_size <= 0.1)\n",
    "cached_price.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|registration|price             |\n",
      "+------------+------------------+\n",
      "|TxRX6kA     |3238.9210526315787|\n",
      "|PX4Aq0t     |3238.9210526315787|\n",
      "|ALS3XKU     |3238.9210526315787|\n",
      "|JBlTWUg     |3238.9210526315787|\n",
      "|b03GIjW     |3238.9210526315787|\n",
      "|atj0cou     |3163.0254237288136|\n",
      "|eFTv6hF     |3163.0254237288136|\n",
      "|Rz1GRum     |3163.0254237288136|\n",
      "|UPDkPxK     |3163.0254237288136|\n",
      "|zmpYII0     |3163.0254237288136|\n",
      "+------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 2.38 ms, sys: 3.74 ms, total: 6.11 ms\n",
      "Wall time: 496 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# cache 한 상태에서 Join 할 때 \n",
    "cached_price.groupBy(\"registration\").agg(avg(\"sale_price\").alias(\"price\")).orderBy(desc(\"price\")).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 5. 캐시 전략\n",
    "##### SparkSQL 은 테이블 단위 캐시와, Lazy 하지 않은 캐시인 점을 이용하면 좀 더 직관적인 관리 및 운영이 가능합니다\n",
    "##### Dataframe, Dataset 은 Lazy Evaluation 인 점을 감안하면, 보다 정교한 개발 및 모듈 개발이 가능합니다 (최적화 관점)\n",
    "##### 메모리에 올라와 있지만 캐시가 어떤 이유로든 삭제된 경우 transformation 연산을 통해 다시 계산됩니다\n",
    "##### 스파크는 모든 persist(), cache() 메소드가 호출될 때마다 전체 메모리를 모니터링합니다\n",
    "##### 해당 노드에 사용되지 않거나 혹은 LRU 알고리즘에 따라 캐시 데이터를 삭제합니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 6. Storage Level \n",
    "> 수준에 따른 리소스 사용 매트릭스\n",
    "\n",
    "```bash\n",
    "Storage Level    Space used  CPU time  In memory  On-disk  Serialized   Recompute some partitions\n",
    "----------------------------------------------------------------------------------------------------\n",
    "MEMORY_ONLY          High        Low       Y          N        N         Y    \n",
    "MEMORY_ONLY_SER      Low         High      Y          N        Y         Y\n",
    "MEMORY_AND_DISK      High        Medium    Some       Some     Some      N\n",
    "MEMORY_AND_DISK_SER  Low         High      Some       Some     Y         N\n",
    "DISK_ONLY            Low         High      N          Y        Y         N\n",
    "```\n",
    "\n",
    "* 스토리지 수준에 따른 구현\n",
    "  * MEMORY_ONLY         → RDD 를 Java Object 로 역직렬화 해서 저장하며 메모리가 부족한 경우 모든 단계를 다시 계산해서 사용합니다\n",
    "  * MEMORY_ONLY_2       → MEMORY_ONLY 와 동일하지만 replica 수가 2입니다\n",
    "  * MEMORY_ONLY_SER     → RDD 의 Java Object 형태로 메모리에 저장하지 않고 직렬화된 객체를 메모리에 저장하여 메모리 사용에 조금 더 효과적입니다\n",
    "  * MEMORY_AND_DISK_SER → 메모리가 부족한 경우 부족한 데이터에 대해 디스크에 저장하되, 자바 객체 대신 직렬화된 데이터를 저장합니다\n",
    "  * DISK_ONLY           → 디스크에만 저장합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
