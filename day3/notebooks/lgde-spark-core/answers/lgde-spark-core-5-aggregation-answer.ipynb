{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5교시 집계 연산\n",
    "\n",
    "### 목차\n",
    "* [1. 집계 함수](#1.-집계-함수)\n",
    "* [2. 그룹 함수](#2.-그룹-함수)\n",
    "* [참고자료](#참고자료)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/21 09:08:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://36066668d2b9:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4b2abdddc0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "\n",
    "# 공통 데이터 위치\n",
    "home_jovyan = \"/home/jovyan\"\n",
    "work_data = f\"{home_jovyan}/work/data\"\n",
    "work_dir=!pwd\n",
    "work_dir = work_dir[0]\n",
    "\n",
    "# 로컬 환경 최적화\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) # the number of partitions to use when shuffling data for joins or aggregations.\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\"\"\" 구매 이력 데이터 \"\"\"\n",
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(f\"{work_data}/retail-data/all\")\n",
    "    .coalesce(5)\n",
    ")\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate   |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
      "|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |12/1/2010 8:26|2.55     |17850     |United Kingdom|\n",
      "|536365   |71053    |WHITE METAL LANTERN                |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n",
      "|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |12/1/2010 8:26|2.75     |17850     |United Kingdom|\n",
      "|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n",
      "|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n",
      "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.show(5, truncate=False)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 집계 함수\n",
    "### 1.1 로우 수 (count, countDistinct, approx_count_distinct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  541909|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    1454|\n",
      "+--------+\n",
      "\n",
      "+------------------+\n",
      "|count(Description)|\n",
      "+------------------+\n",
      "|            540455|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df.selectExpr(\"count(*)\").show()\n",
    "df.where(\"Description is null\").selectExpr(\"count(1)\").show() # 1,454\n",
    "df.selectExpr(\"count(Description)\").show() # 540,455 + 1,454 = 541,909"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n",
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "# 명시적으로 컬럼을 지정한 경우 해당 컬럼이 널 인 경우 해당 로우는 제외됩니다\n",
    "df.select(countDistinct(\"StockCode\")).show()\n",
    "df.selectExpr(\"count(distinct StockCode)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/21 08:49:41 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 20:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            4079|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "# 근사치로 구하지만 연산 속도가 빠름\n",
    "df.select(approx_count_distinct(\"StockCode\", 0.1)).show() # 0.1은 최대 추정 오류율\n",
    "df.select(approx_count_distinct(\"StockCode\", 0.01)).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 수치 집계 함수 (first, last, min, max, sum, sumDistinct, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+\n",
      "|first(StockCode)|last(StockCode)|\n",
      "+----------------+---------------+\n",
      "|          85123A|          22138|\n",
      "+----------------+---------------+\n",
      "\n",
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|    min(Description)| max(Description)|\n",
      "+--------------------+-----------------+\n",
      "| 4 PURPLE FLOCK D...|wrongly sold sets|\n",
      "+--------------------+-----------------+\n",
      "\n",
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n",
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show(1) # null도 감안하려면 True\n",
    "\n",
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show(1)\n",
    "df.select(min(\"Description\"), max(\"Description\")).show(1) # 문자열\n",
    "\n",
    "df.select(sum(\"Quantity\")).show(1)\n",
    "df.select(sumDistinct(\"Quantity\")).show(1) # 고유값을 합산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 통계 집계 함수 (avg, mean, variance, stddev) \n",
    "* 표본표준분산 및 편차: variance, stddev\n",
    "* 모표준분산 및 편차 : var_pop, stddev_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+----------------+-----------------+\n",
      "|(total_purchases / total_transcations)|   avg_purchases|mean_transcations|\n",
      "+--------------------------------------+----------------+-----------------+\n",
      "|                      9.55224954743324|9.55224954743324| 9.55224954743324|\n",
      "+--------------------------------------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df.select(\n",
    "    count(\"Quantity\").alias(\"total_transcations\"),\n",
    "    sum(\"Quantity\").alias(\"total_purchases\"),\n",
    "    avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "    expr(\"mean(Quantity)\").alias(\"mean_transcations\"),    \n",
    ").selectExpr(\n",
    "    \"total_purchases / total_transcations\",\n",
    "    \"avg_purchases\",\n",
    "    \"mean_transcations\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------------+------------------+---------------------+------------------+--------------------+\n",
      "|var_samp(Quantity)|stddev_samp(Quantity)|var_samp(Quantity)|stddev_samp(Quantity)| var_pop(Quantity)|stddev_pop(Quantity)|\n",
      "+------------------+---------------------+------------------+---------------------+------------------+--------------------+\n",
      "|47559.391409298696|   218.08115785023404|47559.391409298696|   218.08115785023404|47559.303646609005|  218.08095663447784|\n",
      "+------------------+---------------------+------------------+---------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    variance(\"Quantity\")\n",
    "    , stddev(\"Quantity\")\n",
    "    , var_samp(\"Quantity\")\n",
    "    , stddev_samp(\"Quantity\")\n",
    "    , var_pop(\"Quantity\")\n",
    "    , stddev_pop(\"Quantity\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 분산과 표준편차\n",
    "+ 표본표준분산 및 편차: variance, stddev\n",
    "+ 모표준분산 및 편차 : var_pop, stddev_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------------+------------------+---------------------+------------------+--------------------+\n",
      "|var_samp(Quantity)|stddev_samp(Quantity)|var_samp(Quantity)|stddev_samp(Quantity)| var_pop(Quantity)|stddev_pop(Quantity)|\n",
      "+------------------+---------------------+------------------+---------------------+------------------+--------------------+\n",
      "|47559.391409298696|   218.08115785023404|47559.391409298696|   218.08115785023404|47559.303646609005|  218.08095663447784|\n",
      "+------------------+---------------------+------------------+---------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import variance, stddev\n",
    "from pyspark.sql.functions import var_samp, stddev_samp\n",
    "from pyspark.sql.functions import var_pop, stddev_pop\n",
    "\n",
    "df.select(variance(\"Quantity\"), stddev(\"Quantity\"),      \n",
    "          var_samp(\"Quantity\"), stddev_samp(\"Quantity\"), # 위와 동일\n",
    "          var_pop(\"Quantity\"), stddev_pop(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:>                                                         (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------------+------------------+---------------------+-----------------+--------------------+\n",
      "|var_samp(Quantity)|stddev_samp(Quantity)|var_samp(Quantity)|stddev_samp(Quantity)|var_pop(Quantity)|stddev_pop(Quantity)|\n",
      "+------------------+---------------------+------------------+---------------------+-----------------+--------------------+\n",
      "|              null|                 null|              null|                 null|              0.0|                 0.0|\n",
      "+------------------+---------------------+------------------+---------------------+-----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(df.select(\"*\").take(1)).select(variance(\"Quantity\"), stddev(\"Quantity\"),      \n",
    "          var_samp(\"Quantity\"), stddev_samp(\"Quantity\"), # 위와 동일\n",
    "          var_pop(\"Quantity\"), stddev_pop(\"Quantity\")).show() # 1일 때는 NaN이 나옵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 비대칭도와 첨도\n",
    "+ 비대칭도와 첨도 : https://www.youtube.com/watch?time_continue=2&v=g9VOhfy2WWY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "| skewness(Quantity)|kurtosis(Quantity)|\n",
      "+-------------------+------------------+\n",
      "|-0.2640755761052783|119768.05495533274|\n",
      "+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "\n",
    "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![왜도](images/func1.png)\n",
    "\n",
    "#### 비대칭도, 왜도 (skewness)\n",
    "> 왜도는 데이터가 대칭이 아닌 정도입니다. 왜도 값(0, 양수 또는 음수)이 데이터 형상에 대한 정보를 나타냅니다.\n",
    "데이터가 대칭에 가까울수록 왜도 값이 0에 근접합니다. 그러나 왜도 부족만으로 정규성을 의미하지는 않습니다.\n",
    "\n",
    "#### 첨도(kurtosis)\n",
    "> 첨도는 분포의 꼬리가 정규 분포와 어떻게 다른지 나타냅니다. 완전히 정규 분포를 따르는 데이터의 첨도 값은 0입니다.\n",
    "분포의 첨도 값이 양수이면 분포의 꼬리가 정규 분포보다 두껍다는 것을 나타냅니다\n",
    "분포의 첨도 값이 음수이면 분포의 꼬리가 정규 분포보다 얇다는 것을 나타냅니다. \n",
    "\n",
    "#### [skewness](https://github.com/apache/spark/blob/5a7403623d0525c23ab8ae575e9d1383e3e10635/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/CentralMomentAgg.scala#L231)\n",
    "```scala\n",
    "org.apache.spark.sql.catalyst.expressions.aggregate.CentralMomentAgg\n",
    "\n",
    "def skewness(columnName: String): Column = skewness(Column(columnName))\n",
    "def skewness(e: Column): Column = withAggregateFunction { Skewness(e.expr) }\n",
    "def kurtosis(e: Column): Column = withAggregateFunction { Kurtosis(e.expr) }\n",
    "def kurtosis(columnName: String): Column = kurtosis(Column(columnName))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 공분산과 상관관계\n",
    "+ 표본공분산(cover_samp), 모공분산(cover_pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------------------------------+-------------------------------+\n",
      "|corr(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|\n",
      "+-------------------------+------------------------------+-------------------------------+\n",
      "|     4.912186085648426E-4|            1052.7260778770628|              1052.728054393167|\n",
      "+-------------------------+------------------------------+-------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr, covar_pop, covar_samp\n",
    "\n",
    "df.select(corr(\"InvoiceNo\", \"Quantity\"), covar_pop(\"InvoiceNo\", \"Quantity\"), covar_samp(\"InvoiceNo\", \"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![공분산](images/func2.gif)\n",
    "          \n",
    "#### [공분산](https://ko.wikipedia.org/wiki/%EA%B3%B5%EB%B6%84%EC%82%B0) (covariance)\n",
    "> 공분산(共分散, 영어: covariance)은 2개의 확률변수의 상관정도를 나타내는 값이다.(1개의 변수의 이산정도를 나타내는 분산과는 별개임) 만약 2개의 변수중 하나의 값이 상승하는 경향을 보일 때, 다른 값도 상승하는 경향의 상관관계에 있다면, 공분산의 값은 양수가 될 것이다. 반대로 2개의 변수중 하나의 값이 상승하는 경향을 보일 때, 다른 값이 하강하는 경향을 보인다면 공분산의 값은 음수가 된다. <br>\n",
    "<br>\n",
    "![function](images/func3.png)\n",
    "\n",
    "<br>\n",
    "단, 100점만점인 두과목의 점수 공분산은 별로 상관성이 부족하지만 100점만점이기 때문에 큰 값이 나오고\n",
    "10점짜리 두과목의 점수 공분산은 상관성이 아주 높을지만 10점만점이기 때문에 작은값이 나온다\n",
    "\n",
    "![function](images/func4.png)\n",
    "\n",
    "#### [상관관계](https://ko.wikipedia.org/wiki/%EC%83%81%EA%B4%80_%EB%B6%84%EC%84%9D) (correlation)\n",
    "> 상관 분석(Correlation analysis)은 확률론과 통계학에서 두 변수간에 어떤 선형적 관계를 갖고 있는 지를 분석하는 방법이다. \n",
    "![function](image/func5.png)\n",
    "\n",
    "#### 피어슨 상관 계수\n",
    "> 피어슨 상관 계수란 두 변수 X 와 Y 간의 선형 상관 관계를 계량화한 수치다 . 피어슨 상관 계수는 코시-슈바르츠 부등식에 의해 +1과 -1 사이의 값을 가지며, +1은 완벽한 양의 선형 상관 관계, 0은 선형 상관 관계 없음, -1은 완벽한 음의 선형 상관 관계를 의미한다.\n",
    "\n",
    "#### Perason's r = X와 Y가 함께 변하는 정도 / X와 Y가 각각 변하는 정도\n",
    "##### r 값은 X 와 Y 가 완전히 동일하면 +1, 전혀 다르면 0, 반대방향으로 완전히 동일 하면 –1 을 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 복합 데이터 타입의 집계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------------+\n",
      "|collect_list(Country)|collect_set(Country)|\n",
      "+---------------------+--------------------+\n",
      "| [United Kingdom, ...|[Portugal, Italy,...|\n",
      "+---------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list, collect_set, size\n",
    "\n",
    "df.select(collect_list(\"Country\"), collect_set(\"Country\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+--------------------------+\n",
      "|size(collect_list(Country))|size(collect_set(Country))|\n",
      "+---------------------------+--------------------------+\n",
      "|                     541909|                        38|\n",
      "+---------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(size(collect_list(\"Country\")), size(collect_set(\"Country\"))).show() # 각 컬럼의 복합데이터 사이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT Country)|\n",
      "+-----------------------+\n",
      "|                     38|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(countDistinct(\"Country\")).show() # 중복없이 카운트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 복합 데이터 타입의 집계 함수(collect_set, collect_list)의 실용 사례?\n",
    "> 데이터의 Cardinality 가 충분히 많지 않은 경우에 하나의 컬럼에 담아 처리하고 싶을 때 활용할 수 있습니다.\n",
    "\n",
    "#### [collect_set](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/collect.scala#L125)\n",
    "```scala\n",
    "org.apache.spark.sql.catalyst.expressions.aggregate.collect.scala\n",
    "case class CollectSet(\n",
    "    child: Expression,\n",
    "    mutableAggBufferOffset: Int = 0,\n",
    "    inputAggBufferOffset: Int = 0) extends Collect[mutable.HashSet[Any]] {\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 그룹 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 표현식을 이용한 그룹화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------------+\n",
      "|InvoiceNo|CustomerId|CountOfQuantity|\n",
      "+---------+----------+---------------+\n",
      "|   536366|     17850|              2|\n",
      "|   536367|     13047|             12|\n",
      "|   536369|     13047|              1|\n",
      "|   536376|     15291|              2|\n",
      "|   536387|     16029|              5|\n",
      "+---------+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "df.printSchema()\n",
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").agg(expr(\"count(Quantity) as CountOfQuantity\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 맵을 이용한 그룹화\n",
    "> 파이선의 딕셔너리 데이터 타입을 활용하여 집계함수의 표현이 가능 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------+------------------+\n",
      "|InvoiceNo|stddev_pop(UnitPrice)|     avg(Quantity)|\n",
      "+---------+---------------------+------------------+\n",
      "|   536370|   3.6916533897428674|             22.45|\n",
      "|   536380|                  0.0|              24.0|\n",
      "|   536384|   3.5529802474898813|14.615384615384615|\n",
      "|   536387|   1.0775602071346178|             288.0|\n",
      "|   536397|                  0.0|              30.0|\n",
      "+---------+---------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg( { \"Quantity\" : \"avg\", \"UnitPrice\" : \"stddev_pop\" } ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 맵을 이용한 그룹화 (agg(key->value))\n",
    "> 맵을 이용하여 컬럼 단위로 적용할 함수를 전달하는 방식입니다\n",
    "\n",
    "#### [RelationalGroupedDataset.agg](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala#L168)\n",
    "```scala\n",
    "org.apache.spark.sql.RelationalGroupedDataset\n",
    "\n",
    "def agg(exprs: Map[String, String]): DataFrame = {\n",
    "    toDF(exprs.map { case (colName, expr) =>\n",
    "        strToExpr(expr)(df(colName).expr)\n",
    "    }.toSeq)\n",
    "}\n",
    "```\n",
    "##### 1. 위의 함수 호출 시에 value (key) 형식으로 expression 을 만들어주게 됩니다.\n",
    "##### 2. map 이 mutable.HashMap  을 넘기면  Compile Error 가 발생함에 유의합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>InvoiceNo</th><th>stddev_pop(Quantity)</th></tr>\n",
       "<tr><td>536370</td><td>8.935742834258381</td></tr>\n",
       "<tr><td>536380</td><td>0.0</td></tr>\n",
       "<tr><td>536384</td><td>15.750645708563392</td></tr>\n",
       "<tr><td>536387</td><td>117.57550765359255</td></tr>\n",
       "<tr><td>536397</td><td>18.0</td></tr>\n",
       "<tr><td>536405</td><td>0.0</td></tr>\n",
       "<tr><td>536407</td><td>0.0</td></tr>\n",
       "<tr><td>536463</td><td>0.0</td></tr>\n",
       "<tr><td>536500</td><td>4.019950248448356</td></tr>\n",
       "<tr><td>536522</td><td>1.6046058535136642</td></tr>\n",
       "<tr><td>536523</td><td>7.487025815072067</td></tr>\n",
       "<tr><td>536536</td><td>34.373762603991366</td></tr>\n",
       "<tr><td>536538</td><td>3.7173833008743054</td></tr>\n",
       "<tr><td>536542</td><td>8.73212459828649</td></tr>\n",
       "<tr><td>536555</td><td>0.0</td></tr>\n",
       "<tr><td>536561</td><td>2.9814239699997196</td></tr>\n",
       "<tr><td>536573</td><td>23.748684174075834</td></tr>\n",
       "<tr><td>536579</td><td>78.0</td></tr>\n",
       "<tr><td>536580</td><td>2.23606797749979</td></tr>\n",
       "<tr><td>536582</td><td>6.99777522727673</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+---------+--------------------+\n",
       "|InvoiceNo|stddev_pop(Quantity)|\n",
       "+---------+--------------------+\n",
       "|   536370|   8.935742834258381|\n",
       "|   536380|                 0.0|\n",
       "|   536384|  15.750645708563392|\n",
       "|   536387|  117.57550765359255|\n",
       "|   536397|                18.0|\n",
       "|   536405|                 0.0|\n",
       "|   536407|                 0.0|\n",
       "|   536463|                 0.0|\n",
       "|   536500|   4.019950248448356|\n",
       "|   536522|  1.6046058535136642|\n",
       "|   536523|   7.487025815072067|\n",
       "|   536536|  34.373762603991366|\n",
       "|   536538|  3.7173833008743054|\n",
       "|   536542|    8.73212459828649|\n",
       "|   536555|                 0.0|\n",
       "|   536561|  2.9814239699997196|\n",
       "|   536573|  23.748684174075834|\n",
       "|   536579|                78.0|\n",
       "|   536580|    2.23606797749979|\n",
       "|   536582|    6.99777522727673|\n",
       "+---------+--------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 런타임 시에 맵으로 전달된 함수를 표현식으로 사용할 수 있습니다.\n",
    "(\n",
    "    df.groupBy(\"InvoiceNo\")\n",
    "    .agg(\n",
    "        {\"Quantity\":\"avg\", \"Quantity\":\"stddev_pop\"}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 요약\n",
    "+ User Defined Aggregation Function, UDAF\n",
    "+ UDAF를 생성하려면 기본 클래스인 UserDefinedAggregateFunction을 상속\n",
    "+ UDAF는 현재 스칼라와 자바로만 사용할 수 있음(ver 2.3)\n",
    "```\n",
    "inputSchema: UDAF 입력 파라미터의 스키마를 StructType으로 정의 \n",
    "bufferSchema: UDAF 중간 결과의 스키마를 StructType으로 정의\n",
    "dataType: 반환될 값의 DataType을 정의\n",
    "deterministic: UDAF가 동일한 입력값에 대해 항상 동일한 결과를 반환하는지 불리언값으로 정의\n",
    "initialize: 집계용 버퍼의 값을 초기화하는 로직을 정의\n",
    "update: 입력받은 로우를 기바느로 내부 버퍼를 업데이트하는 로직을 정의\n",
    "merge: 두 개의 집계용 버퍼를 병합하는 로직을 정의\n",
    "evaluate: 집계의 최종 결과를 생성하는 로직을 정의\n",
    "```\n",
    "\n",
    "※ Efficient UD(A)Fs with PySpark https://www.inovex.de/blog/efficient-udafs-with-pyspark/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>1. [중급]</font> 구매 이력 CSV f\"{work_data}/retail-data/all\" 파일을 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. 데이터 10건을 출력하세요\n",
    "#### 3. 상품코드(StockCode)의 유일한 값의 갯수를 출력하세요\n",
    "#### 4. 상품단가(UnitPrice)의 최소, 최대 값을 출력하세요\n",
    "#### 5. 송장번호(StockCode)별로 송장별총매출금액(TotalInvoicePrice)를 계산하고 내림차순으로 정렬하세요\n",
    "#### 6. 송장별총매출금액(TotalInvoicePrice)이 최고금액이 송장을 필터하여 검증해 보세요\n",
    "##### 예를 들어 `select sum(unit-price * quantity) from table where invoiceno = '123456'` 와 같은 쿼리로 검증이 가능합니다\n",
    "\n",
    "<details><summary>[실습7] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "df1 = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(f\"{work_data}/retail-data/all\")\n",
    ")\n",
    "df1.printSchema()\n",
    "df1.show()\n",
    "answer = df1.withColumn(\"TotalPrice\", expr(\"UnitPrice * Quantity\")).groupBy(\"InvoiceNo\").agg(sum(\"TotalPrice\").alias(\"TotalInvoicePrice\"))\n",
    "answer.printSchema()\n",
    "display(answer.orderBy(desc(\"TotalInvoicePrice\")).limit(10))\n",
    "\n",
    "df1.where(\"InvoiceNo = '581483'\").select(sum(expr(\"UnitPrice * Quantity\"))).show()\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/2010 8:26|     7.65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/2010 8:26|     4.25|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/2010 8:34|     1.69|     13047|United Kingdom|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|12/1/2010 8:34|     3.75|     13047|United Kingdom|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|12/1/2010 8:34|     1.65|     13047|United Kingdom|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|12/1/2010 8:34|     4.25|     13047|United Kingdom|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|12/1/2010 8:34|     4.95|     13047|United Kingdom|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2|12/1/2010 8:34|     9.95|     13047|United Kingdom|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3|12/1/2010 8:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3|12/1/2010 8:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4|12/1/2010 8:34|     7.95|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- TotalInvoicePrice: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>InvoiceNo</th><th>TotalInvoicePrice</th></tr>\n",
       "<tr><td>581483</td><td>168469.6</td></tr>\n",
       "<tr><td>541431</td><td>77183.6</td></tr>\n",
       "<tr><td>574941</td><td>52940.93999999999</td></tr>\n",
       "<tr><td>576365</td><td>50653.909999999996</td></tr>\n",
       "<tr><td>556444</td><td>38970.0</td></tr>\n",
       "<tr><td>567423</td><td>31698.159999999996</td></tr>\n",
       "<tr><td>556917</td><td>22775.930000000008</td></tr>\n",
       "<tr><td>572209</td><td>22206.0</td></tr>\n",
       "<tr><td>567381</td><td>22104.8</td></tr>\n",
       "<tr><td>563614</td><td>21880.439999999995</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+------------------+\n",
       "|InvoiceNo| TotalInvoicePrice|\n",
       "+---------+------------------+\n",
       "|   581483|          168469.6|\n",
       "|   541431|           77183.6|\n",
       "|   574941| 52940.93999999999|\n",
       "|   576365|50653.909999999996|\n",
       "|   556444|           38970.0|\n",
       "|   567423|31698.159999999996|\n",
       "|   556917|22775.930000000008|\n",
       "|   572209|           22206.0|\n",
       "|   567381|           22104.8|\n",
       "|   563614|21880.439999999995|\n",
       "+---------+------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|sum((UnitPrice * Quantity))|\n",
      "+---------------------------+\n",
      "|                   168469.6|\n",
      "+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df1 = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(f\"{work_data}/retail-data/all\")\n",
    ")\n",
    "df1.printSchema()\n",
    "df1.show()\n",
    "answer = df1.withColumn(\"TotalPrice\", expr(\"UnitPrice * Quantity\")).groupBy(\"InvoiceNo\").agg(sum(\"TotalPrice\").alias(\"TotalInvoicePrice\"))\n",
    "answer.printSchema()\n",
    "display(answer.orderBy(desc(\"TotalInvoicePrice\")).limit(10))\n",
    "\n",
    "df1.where(\"InvoiceNo = '581483'\").select(sum(expr(\"UnitPrice * Quantity\"))).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>2. [기본]</font> 매출 테이블 f\"{work_data}/tbl_purchase.csv\" CSV 파일을 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. 데이터 10건을 출력하세요\n",
    "#### 3. 제품(p_name)별 금액(p_amount) 의 전체 합인 총 매출금액(sum_amount)을 구하세요\n",
    "\n",
    "<details><summary>[실습2] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "df2 = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(f\"{work_data}/tbl_purchase.csv\")\n",
    ")\n",
    "df2.printSchema()\n",
    "df2.show()\n",
    "answer = df2.groupBy(\"p_name\").agg(sum(\"p_amount\").alias(\"sum_amount\"))\n",
    "answer.printSchema()\n",
    "display(answer)\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- p_time: integer (nullable = true)\n",
      " |-- p_uid: integer (nullable = true)\n",
      " |-- p_id: integer (nullable = true)\n",
      " |-- p_name: string (nullable = true)\n",
      " |-- p_amount: integer (nullable = true)\n",
      "\n",
      "+----------+-----+----+-----------+--------+\n",
      "|    p_time|p_uid|p_id|     p_name|p_amount|\n",
      "+----------+-----+----+-----------+--------+\n",
      "|1603651550|    0|1000|GoldStar TV|  100000|\n",
      "|1603651550|    1|2000|    LG DIOS| 2000000|\n",
      "|1603694755|    1|2001|    LG Gram| 1800000|\n",
      "|1603673500|    2|2002|    LG Cyon| 1400000|\n",
      "|1603652155|    3|2003|      LG TV| 1000000|\n",
      "|1603674500|    4|2004|LG Computer| 4500000|\n",
      "|1603665955|    5|2001|    LG Gram| 3500000|\n",
      "|1603666155|    5|2003|      LG TV| 2500000|\n",
      "+----------+-----+----+-----------+--------+\n",
      "\n",
      "root\n",
      " |-- p_name: string (nullable = true)\n",
      " |-- sum_amount: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>p_name</th><th>sum_amount</th></tr>\n",
       "<tr><td>LG Cyon</td><td>1400000</td></tr>\n",
       "<tr><td>LG Gram</td><td>5300000</td></tr>\n",
       "<tr><td>LG Computer</td><td>4500000</td></tr>\n",
       "<tr><td>LG TV</td><td>3500000</td></tr>\n",
       "<tr><td>GoldStar TV</td><td>100000</td></tr>\n",
       "<tr><td>LG DIOS</td><td>2000000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------+----------+\n",
       "|     p_name|sum_amount|\n",
       "+-----------+----------+\n",
       "|    LG Cyon|   1400000|\n",
       "|    LG Gram|   5300000|\n",
       "|LG Computer|   4500000|\n",
       "|      LG TV|   3500000|\n",
       "|GoldStar TV|    100000|\n",
       "|    LG DIOS|   2000000|\n",
       "+-----------+----------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df2 = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(f\"{work_data}/tbl_purchase.csv\")\n",
    ")\n",
    "df2.printSchema()\n",
    "df2.show()\n",
    "answer = df2.groupBy(\"p_name\").agg(sum(\"p_amount\").alias(\"sum_amount\"))\n",
    "answer.printSchema()\n",
    "display(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>3. [기본]</font> 매출 테이블 f\"{work_data}/tbl_purchase.csv\" CSV 파일을 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. 데이터 10건을 출력하세요\n",
    "#### 3. 구매 금액의 합이 가장 높은 고객(p_uid)을 구하세요\n",
    "\n",
    "<details><summary>[실습3] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "df3 = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(f\"{work_data}/tbl_purchase.csv\")\n",
    ")\n",
    "df3.printSchema()\n",
    "df3.show()\n",
    "answer = df2.groupBy(\"p_uid\").agg(sum(\"p_amount\").alias(\"sum_amount_per_user\"))\n",
    "answer.printSchema()\n",
    "display(answer)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- p_time: integer (nullable = true)\n",
      " |-- p_uid: integer (nullable = true)\n",
      " |-- p_id: integer (nullable = true)\n",
      " |-- p_name: string (nullable = true)\n",
      " |-- p_amount: integer (nullable = true)\n",
      "\n",
      "+----------+-----+----+-----------+--------+\n",
      "|    p_time|p_uid|p_id|     p_name|p_amount|\n",
      "+----------+-----+----+-----------+--------+\n",
      "|1603651550|    0|1000|GoldStar TV|  100000|\n",
      "|1603651550|    1|2000|    LG DIOS| 2000000|\n",
      "|1603694755|    1|2001|    LG Gram| 1800000|\n",
      "|1603673500|    2|2002|    LG Cyon| 1400000|\n",
      "|1603652155|    3|2003|      LG TV| 1000000|\n",
      "|1603674500|    4|2004|LG Computer| 4500000|\n",
      "|1603665955|    5|2001|    LG Gram| 3500000|\n",
      "|1603666155|    5|2003|      LG TV| 2500000|\n",
      "+----------+-----+----+-----------+--------+\n",
      "\n",
      "root\n",
      " |-- p_uid: integer (nullable = true)\n",
      " |-- sum_amount_per_user: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>p_uid</th><th>sum_amount_per_user</th></tr>\n",
       "<tr><td>0</td><td>100000</td></tr>\n",
       "<tr><td>3</td><td>1000000</td></tr>\n",
       "<tr><td>5</td><td>6000000</td></tr>\n",
       "<tr><td>4</td><td>4500000</td></tr>\n",
       "<tr><td>1</td><td>3800000</td></tr>\n",
       "<tr><td>2</td><td>1400000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+-------------------+\n",
       "|p_uid|sum_amount_per_user|\n",
       "+-----+-------------------+\n",
       "|    0|             100000|\n",
       "|    3|            1000000|\n",
       "|    5|            6000000|\n",
       "|    4|            4500000|\n",
       "|    1|            3800000|\n",
       "|    2|            1400000|\n",
       "+-----+-------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df3 = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(f\"{work_data}/tbl_purchase.csv\")\n",
    ")\n",
    "df3.printSchema()\n",
    "df3.show()\n",
    "answer = df2.groupBy(\"p_uid\").agg(sum(\"p_amount\").alias(\"sum_amount_per_user\"))\n",
    "answer.printSchema()\n",
    "display(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>4. [고급]</font> 샌프란시스코 긴급출동 데이터 CSV 파일인 f\"{work_data}/learning-spark/sf-fire-calls.csv\"를 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. 데이터를 3건 출력하세요\n",
    "#### 3. 호출의 종류(CallType)가 어떤 것들이 있는지 출력하세요 (중복제거)\n",
    "#### 3. 샌프란시스코에서 발생의 가장 빈도수가 높은 종류(CallType)를 구하고 빈도수를 구하세요\n",
    "#### 4. 샌프란시스코에서 발생하는 최고 빈도수 3건은 무엇인가요? \n",
    "\n",
    "<details><summary>[실습3] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "df3 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(f\"{work_data}/learning-spark/sf-fire-calls.csv\")\n",
    ")\n",
    "df3.printSchema()\n",
    "df3.show(3)\n",
    "df3.createOrReplaceTempView(\"fire_calls\")\n",
    "spark.sql(\"select distinct(CallType) from fire_calls\").show(truncate=False)\n",
    "\n",
    "answer = spark.sql(\"select CallType, count(CallType) as CallTypeCount from fire_calls group by CallType order by CallTypeCount desc\")\n",
    "display(answer.limit(3))\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CallNumber: integer (nullable = true)\n",
      " |-- UnitID: string (nullable = true)\n",
      " |-- IncidentNumber: integer (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- CallDate: string (nullable = true)\n",
      " |-- WatchDate: string (nullable = true)\n",
      " |-- CallFinalDisposition: string (nullable = true)\n",
      " |-- AvailableDtTm: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- OriginalPriority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- FinalPriority: integer (nullable = true)\n",
      " |-- ALSUnit: boolean (nullable = true)\n",
      " |-- CallTypeGroup: string (nullable = true)\n",
      " |-- NumAlarms: integer (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- UnitSequenceInCallDispatch: integer (nullable = true)\n",
      " |-- FirePreventionDistrict: string (nullable = true)\n",
      " |-- SupervisorDistrict: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      " |-- Delay: double (nullable = true)\n",
      "\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n",
      "|CallNumber|UnitID|IncidentNumber|        CallType|  CallDate| WatchDate|CallFinalDisposition|       AvailableDtTm|             Address|City|Zipcode|Battalion|StationArea| Box|OriginalPriority|Priority|FinalPriority|ALSUnit|CallTypeGroup|NumAlarms|UnitType|UnitSequenceInCallDispatch|FirePreventionDistrict|SupervisorDistrict|        Neighborhood|            Location|        RowID|    Delay|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n",
      "|  20110016|   T13|       2003235|  Structure Fire|01/11/2002|01/10/2002|               Other|01/11/2002 01:51:...|2000 Block of CAL...|  SF|  94109|      B04|         38|3362|               3|       3|            3|  false|         null|        1|   TRUCK|                         2|                     4|                 5|     Pacific Heights|(37.7895840679362...|020110016-T13|     2.95|\n",
      "|  20110022|   M17|       2003241|Medical Incident|01/11/2002|01/10/2002|               Other|01/11/2002 03:01:...|0 Block of SILVER...|  SF|  94124|      B10|         42|6495|               3|       3|            3|   true|         null|        1|   MEDIC|                         1|                    10|                10|Bayview Hunters P...|(37.7337623673897...|020110022-M17|      4.7|\n",
      "|  20110023|   M41|       2003242|Medical Incident|01/11/2002|01/10/2002|               Other|01/11/2002 02:39:...|MARKET ST/MCALLIS...|  SF|  94102|      B03|         01|1455|               3|       3|            3|   true|         null|        1|   MEDIC|                         2|                     3|                 6|          Tenderloin|(37.7811772186856...|020110023-M41|2.4333334|\n",
      "|  20110032|   E11|       2003250|    Vehicle Fire|01/11/2002|01/10/2002|               Other|01/11/2002 04:16:...|APPLETON AV/MISSI...|  SF|  94110|      B06|         32|5626|               3|       3|            3|  false|         null|        1|  ENGINE|                         1|                     6|                 9|      Bernal Heights|(37.7388432849018...|020110032-E11|      1.5|\n",
      "|  20110043|   B04|       2003259|          Alarms|01/11/2002|01/10/2002|               Other|01/11/2002 06:01:...|1400 Block of SUT...|  SF|  94109|      B04|         03|3223|               3|       3|            3|  false|         null|        1|   CHIEF|                         2|                     4|                 2|    Western Addition|(37.7872890372638...|020110043-B04|3.4833333|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------------------------------+\n",
      "|CallType                                    |\n",
      "+--------------------------------------------+\n",
      "|Vehicle Fire                                |\n",
      "|Alarms                                      |\n",
      "|Elevator / Escalator Rescue                 |\n",
      "|Explosion                                   |\n",
      "|Assist Police                               |\n",
      "|Marine Fire                                 |\n",
      "|Outside Fire                                |\n",
      "|Water Rescue                                |\n",
      "|Gas Leak (Natural and LP Gases)             |\n",
      "|Aircraft Emergency                          |\n",
      "|HazMat                                      |\n",
      "|Train / Rail Incident                       |\n",
      "|Extrication / Entrapped (Machinery, Vehicle)|\n",
      "|Medical Incident                            |\n",
      "|Smoke Investigation (Outside)               |\n",
      "|Fuel Spill                                  |\n",
      "|Oil Spill                                   |\n",
      "|Watercraft in Distress                      |\n",
      "|Confined Space / Structure Collapse         |\n",
      "|Structure Fire                              |\n",
      "+--------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>CallType</th><th>CallTypeCount</th></tr>\n",
       "<tr><td>Medical Incident</td><td>113794</td></tr>\n",
       "<tr><td>Structure Fire</td><td>23319</td></tr>\n",
       "<tr><td>Alarms</td><td>19406</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------------+-------------+\n",
       "|        CallType|CallTypeCount|\n",
       "+----------------+-------------+\n",
       "|Medical Incident|       113794|\n",
       "|  Structure Fire|        23319|\n",
       "|          Alarms|        19406|\n",
       "+----------------+-------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df3 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(f\"{work_data}/learning-spark/sf-fire-calls.csv\")\n",
    ")\n",
    "df3.printSchema()\n",
    "df3.show(5)\n",
    "df3.createOrReplaceTempView(\"fire_calls\")\n",
    "spark.sql(\"select distinct(CallType) from fire_calls\").show(truncate=False)\n",
    "\n",
    "answer = spark.sql(\"select CallType, count(CallType) as CallTypeCount from fire_calls group by CallType order by CallTypeCount desc\")\n",
    "display(answer.limit(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>5. [고급]</font> 샌프란시스코 긴급출동 데이터 CSV 파일인 f\"{work_data}/learning-spark/sf-fire-calls.csv\"를 읽고 다음과 같은 질문도 실습해 보면 재미있을 것 같습니다\n",
    "#### 1. 2018 년의 모든 화재 신고 유형은 무엇 이었습니까?\n",
    "#### 2. 2018 년의 몇 월에 화재 신고가 가장 많았습니까?\n",
    "#### 3. 샌프란시스코에서 2018 년에 가장 많은 화재 신고가 발생한 지역은 어디입니까?\n",
    "#### 4. 2018 년에 화재 신고에 대한 응답 시간이 가장 나쁜 지역은 어디입니까?\n",
    "#### 5. 2018 년 중 어느 주에 화재 신고가 가장 많았습니까?\n",
    "#### 6. 이웃, 우편 번호, 화재 전화 건수간에 상관 관계가 있습니까?\n",
    "#### 7. Parquet 파일 또는 SQL 테이블을 사용하여이 데이터를 저장하고 다시 읽을 수있는 방법은 무엇입니까?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고자료\n",
    "\n",
    "#### 1. [Spark Programming Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "#### 2. [PySpark SQL Modules Documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html)\n",
    "#### 3. <a href=\"https://spark.apache.org/docs/3.0.1/api/sql/\" target=\"_blank\">PySpark 3.0.1 Builtin Functions</a>\n",
    "#### 4. [PySpark Search](https://spark.apache.org/docs/latest/api/python/search.html)\n",
    "#### 5. [Pyspark Functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?#module-pyspark.sql.functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkContext vs. SparkSession (1/2)\n",
    "> SparkContext 는 spark.core 프로젝트이고, SparkSession 은 spark.sql 프로젝트이다\n",
    "아래와 같이 SparkContext 는 Spark 실행에 가장 중심이 되는 객체이고, 병렬화, 브로드캐스팅, 분산 파일 추가 및 종료 등의 스파크 작업을 관장하는 클래스라 볼 수 있습니다\n",
    "\n",
    "#### 1. [SparkContext.{parallelize, broadcast, addFile, listFiles, addJar, listJars, stop, getOrCreate}](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala)\n",
    "```scala\n",
    "\n",
    "'SparkContext : \"Main entry point for Spark functionality. A SparkContext represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster.\"\n",
    "@note Only one `SparkContext` should be active per JVM. You must `stop()` the active `SparkContext` before creating a new one.\n",
    "@param config a Spark Config object describing the application configuration. Any settings in this config overrides the default configs as well as system properties.\n",
    "\n",
    "\n",
    "'parallelize : \"Distribute a local Scala collection to form an RDD.\"\n",
    "@note Parallelize acts lazily. If `seq` is a mutable collection and is altered after the call to parallelize and before the first action on the RDD, the resultant RDD will reflect the modified collection. Pass a copy of the argument to avoid this.\n",
    "@note avoid using `parallelize(Seq())` to create an empty `RDD`. Consider `emptyRDD` for an RDD with no partitions, or `parallelize(Seq[T]())` for an RDD of `T` with empty partitions.\n",
    "@param seq Scala collection to distribute\n",
    "@param numSlices number of partitions to divide the collection into\n",
    "def parallelize[T: ClassTag](seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope {\n",
    "  assertNotStopped()\n",
    "  new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())\n",
    "}\n",
    "\n",
    "'broadcast : \"Broadcast a read-only variable to the cluster, returning a [[org.apache.spark.broadcast.Broadcast]] object for reading it in distributed functions. The variable will be sent to each cluster only once.\"\n",
    "@param value value to broadcast to the Spark nodes\n",
    "@return `Broadcast` object, a read-only variable cached on each machine\n",
    "def broadcast[T: ClassTag](value: T): Broadcast[T] = {\n",
    "  assertNotStopped()\n",
    "  require(!classOf[RDD[_]].isAssignableFrom(classTag[T].runtimeClass), \"Can not directly broadcast RDDs; instead, call collect() and broadcast the result.\")\n",
    "  val bc = env.broadcastManager.newBroadcast[T](value, isLocal)\n",
    "  val callSite = getCallSite\n",
    "  logInfo(\"Created broadcast \" + bc.id + \" from \" + callSite.shortForm)\n",
    "  cleaner.foreach(_.registerBroadcastForCleanup(bc))\n",
    "  bc\n",
    "}\n",
    "\n",
    "'addFile : \" Add a file to be downloaded with this Spark job on every node. If a file is added during execution, it will not be available until the next TaskSet starts.\"\n",
    "@param path can be either a local file, a file in HDFS (or other Hadoop-supported filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs, use `SparkFiles.get(fileName)` to find its download location. \n",
    "@note A path can be added only once. Subsequent additions of the same path are ignored.\n",
    "def addFile(path: String): Unit = {\n",
    "  addFile(path, false)\n",
    "}\n",
    "\n",
    "'listFiles : \"Returns a list of file paths that are added to resources.\"\n",
    "def listFiles(): Seq[String] = addedFiles.keySet.toSeq\n",
    "\n",
    "'addJar : \"Adds a JAR dependency for all tasks to be executed on this `SparkContext` in the future.  If a jar is added during execution, it will not be available until the next TaskSet starts.\"\n",
    "@param path can be either a local file, a file in HDFS (or other Hadoop-supported filesystems), an HTTP, HTTPS or FTP URI, or local:/path for a file on every worker node.\n",
    "@note A path can be added only once. Subsequent additions of the same path are ignored.\n",
    "def addJar(path: String) { ... }\n",
    "\n",
    "'listJars : \"Returns a list of jar files that are added to resources.\"\n",
    "def listJars(): Seq[String] = addedJars.keySet.toSeq\n",
    "\n",
    "\n",
    "'stop : \"Shut down the SparkContext.\"\n",
    "def stop(): Unit = { ... }\n",
    "\n",
    "'getOrCreate(config) : \"This function may be used to get or instantiate a SparkContext and register it as a singleton object. Because we can only have one active SparkContext per JVM, this is useful when applications may wish to share a SparkContext.\"\n",
    "@param config `SparkConfig` that will be used for initialisation of the `SparkContext`\n",
    "@return current `SparkContext` (or a new one if it wasn't created before the function call)\n",
    "def getOrCreate(config: SparkConf): SparkContext = {\n",
    "  // Synchronize to ensure that multiple create requests don't trigger an exception\n",
    "  // from assertNoOtherContextIsRunning within setActiveContext\n",
    "  SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized {\n",
    "    if (activeContext.get() == null) {\n",
    "      setActiveContext(new SparkContext(config))\n",
    "    } else {\n",
    "      if (config.getAll.nonEmpty) { logWarning(\"Using an existing SparkContext; some configuration may not take effect.\") }\n",
    "    }\n",
    "    activeContext.get()\n",
    "  }\n",
    "}\n",
    "\n",
    "'getOrCreate() : \"This function may be used to get or instantiate a SparkContext and register it as a singleton object. Because we can only have one active SparkContext per JVM, this is useful when applications may wish to share a SparkContext. This method allows not passing a SparkConf (useful if just retrieving).\"\n",
    "@return current `SparkContext` (or a new one if wasn't created before the function call)\n",
    "def getOrCreate(): SparkContext = {\n",
    "  SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized {\n",
    "    if (activeContext.get() == null) { setActiveContext(new SparkContext()) }\n",
    "    activeContext.get()\n",
    "  }\n",
    "}\n",
    "```\n",
    "> Only one `SparkContext` should be active per JVM. You must `stop()` the active `SparkContext` before creating a new one. <br>\n",
    "a Spark Config object describing the application configuration. Any settings in this config overrides the default configs as well as system properties.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkContext vs. SparkSession (2/2)\n",
    "> SparkSession 경우 sparkContext 와 관련을 가지는 객체와 데이터프레임을 다루는 함수들로 구성되어 있습니다.\n",
    "\n",
    "#### 2. [SparkSession](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala)\n",
    "\n",
    "#### 2-1 SparkSession.{version, sqlContext, udf}\n",
    "#### 2-2 SparkSession.{newSession, emptyDataFrame, emptyDataset, createDataFrame}\n",
    "#### 2-3 SparkSession.{range, table, sql, read, time, stop, close}\n",
    "#### 2-4 SparkSession.builder.{master, appName, config, getOrCreate}\n",
    "\n",
    "```scala\n",
    "'SparkSession : \"The entry point to programming Spark with the Dataset and DataFrame API.  In environments that this has been created upfront (e.g. REPL, notebooks), use the builder to get an existing session:\"\n",
    "SparkSession.builder().getOrCreate()\n",
    "\"The builder can also be used to create a new session:\"\n",
    "SparkSession.builder\n",
    "  .master(\"local\")\n",
    "  .appName(\"Word Count\")\n",
    "  .config(\"spark.some.config.option\", \"some-value\")\n",
    "  .getOrCreate()\n",
    "@param sparkContext The Spark context associated with this Spark session.\n",
    "@param existingSharedState If supplied, use the existing shared state instead of creating a new one.\n",
    "@param parentSessionState If supplied, inherit all session state (i.e. temporary views, SQL config, UDFs etc) from parent.\n",
    "\n",
    "\n",
    "'version : \"The version of Spark on which this application is running.\"\n",
    "def version: String = SPARK_VERSION\n",
    "\n",
    "'sqlContext : \"A wrapped version of this session in the form of a [[SQLContext]], for backward compatibility.\"\n",
    "val sqlContext: SQLContext = new SQLContext(this)\n",
    "\n",
    "'udf : \"A collection of methods for registering user-defined functions (UDF).  The following example registers a Scala closure as UDF:\"\n",
    "sparkSession.udf.register(\"myUDF\", (arg1: Int, arg2: String) => arg2 + arg1)\n",
    "\"The following example registers a UDF in Java:\"\n",
    "sparkSession.udf().register(\"myUDF\",\n",
    "   (Integer arg1, String arg2) -> arg2 + arg1,\n",
    "   DataTypes.StringType);\n",
    "@note The user-defined functions must be deterministic. Due to optimization, duplicate invocations may be eliminated or the function may even be invoked more times than it is present in the query.\n",
    "def udf: UDFRegistration = sessionState.udfRegistration\n",
    "\n",
    "'newSession : \"Start a new session with isolated SQL configurations, temporary tables, registered functions are isolated, but sharing the underlying `SparkContext` and cached data.\"\n",
    "@note Other than the `SparkContext`, all shared state is initialized lazily. This method will force the initialization of the shared state to ensure that parent and child sessions are set up with the same shared state. If the underlying catalog implementation is Hive, this will initialize the metastore, which may take some time.\n",
    "def newSession(): SparkSession = {\n",
    "  new SparkSession(sparkContext, Some(sharedState), parentSessionState = None, extensions)\n",
    "}\n",
    "\n",
    "'emptyDataFrame : \"Returns a `DataFrame` with no rows or columns.\"\n",
    "lazy val emptyDataFrame: DataFrame = {\n",
    "  createDataFrame(sparkContext.emptyRDD[Row].setName(\"empty\"), StructType(Nil))\n",
    "}\n",
    "\n",
    "'emptyDataset : \"Creates a new [[Dataset]] of type T containing zero elements.\"\n",
    "def emptyDataset[T: Encoder]: Dataset[T] = {\n",
    "  val encoder = implicitly[Encoder[T]]\n",
    "  new Dataset(self, LocalRelation(encoder.schema.toAttributes), encoder)\n",
    "}\n",
    "\n",
    "'createDataFrame : \"Creates a `DataFrame` from an RDD of Product (e.g. case classes, tuples).\"\n",
    "def createDataFrame[A <: Product : TypeTag](rdd: RDD[A]): DataFrame = {\n",
    "  SparkSession.setActiveSession(this)\n",
    "  val encoder = Encoders.product[A]\n",
    "  Dataset.ofRows(self, ExternalRDD(rdd, self)(encoder))\n",
    "}\n",
    "\n",
    "'range(end) : \"Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements in a range from 0 to `end` (exclusive) with step value 1.\"\n",
    "def range(end: Long): Dataset[java.lang.Long] = range(0, end)\n",
    "\n",
    "'range(start, end) : \"Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements in a range from `start` to `end` (exclusive) with step value 1.\"\n",
    "def range(start: Long, end: Long): Dataset[java.lang.Long] = {\n",
    "  range(start, end, step = 1, numPartitions = sparkContext.defaultParallelism)\n",
    "}\n",
    "\n",
    "'table(tableName) : \"Returns the specified table/view as a `DataFrame`.\"\n",
    "@param tableName is either a qualified or unqualified name that designates a table or view. If a database is specified, it identifies the table/view from the database. Otherwise, it first attempts to find a temporary view with the given name and then match the table/view from the current database. Note that, the global temporary view database is also valid here.\n",
    "def table(tableName: String): DataFrame = {\n",
    "  table(sessionState.sqlParser.parseMultipartIdentifier(tableName))\n",
    "}\n",
    "\n",
    "'sql : \"Executes a SQL query using Spark, returning the result as a `DataFrame`. The dialect that is used for SQL parsing can be configured with 'spark.sql.dialect'.\"\n",
    "def sql(sqlText: String): DataFrame = {\n",
    "  val tracker = new QueryPlanningTracker\n",
    "  val plan = tracker.measurePhase(QueryPlanningTracker.PARSING) {\n",
    "    sessionState.sqlParser.parsePlan(sqlText)\n",
    "  }\n",
    "  Dataset.ofRows(self, plan, tracker)\n",
    "}\n",
    "\n",
    "'read : \"Returns a [[DataFrameReader]] that can be used to read non-streaming data in as a `DataFrame`.\"\n",
    "sparkSession.read.parquet(\"/path/to/file.parquet\")\n",
    "sparkSession.read.schema(schema).json(\"/path/to/file.json\")\n",
    "def read: DataFrameReader = new DataFrameReader(self)\n",
    "\n",
    "'time : \"Executes some code block and prints to stdout the time taken to execute the block. This is available in Scala only and is used primarily for interactive testing and debugging.\"\n",
    "def time[T](f: => T): T = {\n",
    "  val start = System.nanoTime()\n",
    "  val ret = f\n",
    "  val end = System.nanoTime()\n",
    "  // scalastyle:off println\n",
    "  println(s\"Time taken: {NANOSECONDS.toMillis(end - start)} ms\")\n",
    "  // scalastyle:on println\n",
    "  ret\n",
    "}\n",
    "\n",
    "'stop : \"Stop the underlying `SparkContext`.\"\n",
    "def stop(): Unit = { sparkContext.stop() }\n",
    "\n",
    "'close : \"Synonym for `stop()`.\"\n",
    "override def close(): Unit = stop()\n",
    "\n",
    "'SparkSession : \"\"\n",
    "\n",
    "object SparkSession extends Logging {\n",
    "    class Builder extends Logging {\n",
    "        'appName : \" Sets a name for the application, which will be shown in the Spark web UI.\"\n",
    "                \"If no application name is set, a randomly generated name will be used.\"\n",
    "        def appName(name: String): Builder = config(\"spark.app.name\", name)\n",
    "\n",
    "        'getOrCreate : \"Gets an existing [[SparkSession]] or, if there is no existing one, creates a new\n",
    "                one based on the options set in this builder.\n",
    "                This method first checks whether there is a valid thread-local SparkSession,\n",
    "                and if yes, return that one. It then checks whether there is a valid global\n",
    "                default SparkSession, and if yes, return that one. If no valid global default\n",
    "                SparkSession exists, the method creates a new SparkSession and assigns the\n",
    "                newly created SparkSession as the global default.\n",
    "                In case an existing SparkSession is returned, the config options specified in\n",
    "                this builder will be applied to the existing SparkSession.\"\n",
    "        def getOrCreate(): SparkSession = synchronized {\n",
    "            assertOnDriver()\n",
    "        'builder : \"Creates a [[SparkSession.Builder]] for constructing a [[SparkSession]].\"\n",
    "        def builder(): Builder = new Builder\n",
    "    }\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
