{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 스파크 스트리밍 실습 4교시 : 외부 인터페이스\n",
    "\n",
    "> 파일, 카프카 등의 외부 소스로부터 데이터를 읽고, 카산드라 혹은 MySQL 등의 외부 저장소에 저장하는 실습을 합니다\n",
    "\n",
    "## 학습 목표\n",
    "* `File`에 저장된 데이터를 처리합니다\n",
    "  - 로컬 JSON 파일을 직접 생성 및 수정을 통해 스트리밍 데이터를 생성합니다\n",
    "* `Kafka`에 저장된 스트리밍 데이터를 처리합니다\n",
    "  - 카프카 메시지 프로듀서를 통해서 카프카에 스트림 데이터를 생성합니다\n",
    "* `Cassandra`에 스트림 데이터를 저장합니다\n",
    "  - 로컬 JSON 파일을 읽어서 처리하는 애플리케이션을 구현합니다\n",
    "  - 직접 연동이 어렵기 때문에 foreachBatch 메소드를 통해 카산드라에 저장합니다\n",
    "* `MySQL`에 스트림 데이터를 저장합니다\n",
    "  - 로컬 JSON 파일을 읽어서 처리하는 애플리케이션을 구현합니다\n",
    "  - 레코드 단위의 트랜잭션 지원을 위해 foreach 메소드를 통해 MySQL에 저장합니다\n",
    "\n",
    "## 목차\n",
    "* [1. File 소스 스트리밍 실습](#1.-File-소스-스트리밍-실습)\n",
    "* [2. Kafka 소스 스트리밍 실습](#2.-Kafka-소스-스트리밍-실습)\n",
    "* [3. Cassandra 싱크 스트리밍 실습](#3.-Cassandra-싱크-스트리밍-실습)\n",
    "* [4. MySQL 싱크 스트리밍 실습](#4.-MySQL-싱크-스트리밍-실습)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/09 11:49:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://d1067dc1417f:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff15d957dc0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "\n",
    "# 공통 데이터 위치\n",
    "home_jovyan = \"/home/jovyan\"\n",
    "work_data = f\"{home_jovyan}/work/data\"\n",
    "work_dir=!pwd\n",
    "work_dir = work_dir[0]\n",
    "\n",
    "# 로컬 환경 최적화\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) # the number of partitions to use when shuffling data for joins or aggregations.\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "\n",
    "# 현재 기동된 스파크 애플리케이션의 포트를 확인하기 위해 스파크 정보를 출력합니다\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트림 테이블을 주기적으로 조회하는 함수 (name: 이름, sql: Spark SQL, iterations: 반복횟수, sleep_secs: 인터벌)\n",
    "def displayStream(name, sql, iterations, sleep_secs):\n",
    "    from time import sleep\n",
    "    i = 1\n",
    "    for x in range(iterations):\n",
    "        clear_output(wait=True)              # 출력 Cell 을 지웁니다\n",
    "        display('[' + name + '] Iteration: '+str(i)+', Query: '+sql)\n",
    "        display(spark.sql(sql))              # Spark SQL 을 수행합니다\n",
    "        sleep(sleep_secs)                    # sleep_secs 초 만큼 대기합니다\n",
    "        i += 1\n",
    "\n",
    "# 스트림 쿼리의 상태를 주기적으로 조회하는 함수 (name: 이름, query: Streaming Query, iterations: 반복횟수, sleep_secs: 인터벌)\n",
    "def displayStatus(name, query, iterations, sleep_secs):\n",
    "    from time import sleep\n",
    "    i = 1\n",
    "    for x in range(iterations):\n",
    "        clear_output(wait=True)      # Output Cell 의 내용을 지웁니다\n",
    "        display('[' + name + '] Iteration: '+str(i)+', Status: '+query.status['message'])\n",
    "        display(query.lastProgress)  # 마지막 수행된 쿼리의 상태를 출력합니다\n",
    "        sleep(sleep_secs)            # 지정된 시간(초)을 대기합니다\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. File 소스 스트리밍 실습\n",
    "\n",
    "> 기본으로 제공되는 stream 싱크와 소스를 이용하여 데이터 프레임 생성이 가능하며 SparkSession.readStream(), DataFrame.writeStream() 을 이용합니다\n",
    "\n",
    "* 파일로부터 읽기\n",
    "  - 기본적으로 텍스트 파일(CSV, TSV 등)의 경우 스키마가 없는 경우가 많기 때문에 스키마를 직접 생성해 줍니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: long (nullable = true)\n",
      " |-- emp_name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>emp_id</th><th>emp_name</th></tr>\n",
       "<tr><td>1</td><td>OO전자</td></tr>\n",
       "<tr><td>2</td><td>OO화학</td></tr>\n",
       "<tr><td>3</td><td>OO에너지솔루션</td></tr>\n",
       "<tr><td>100</td><td>OO</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------+--------------+\n",
       "|emp_id|      emp_name|\n",
       "+------+--------------+\n",
       "|     1|        OO전자|\n",
       "|     2|        OO화학|\n",
       "|     3|OO에너지솔루션|\n",
       "|   100|            OO|\n",
       "+------+--------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filePath = f\"{work_data}/json\"\n",
    "fileJson = spark.read.json(filePath)\n",
    "fileJson.printSchema() # 기본적으로 스키마를 추론하는 경우 integer 가 아니라 long 으로 잡는다\n",
    "display(fileJson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[fileSource] Iteration: 3, Query: select * from fileSource'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>emp_id</th><th>emp_name</th></tr>\n",
       "<tr><td>1</td><td>OO전자</td></tr>\n",
       "<tr><td>2</td><td>OO화학</td></tr>\n",
       "<tr><td>3</td><td>OO에너지솔루션</td></tr>\n",
       "<tr><td>100</td><td>OO</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------+--------------+\n",
       "|emp_id|      emp_name|\n",
       "+------+--------------+\n",
       "|     1|        OO전자|\n",
       "|     2|        OO화학|\n",
       "|     3|OO에너지솔루션|\n",
       "|   100|            OO|\n",
       "+------+--------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filePath = f\"{work_data}/json\"\n",
    "queryName = \"fileSource\"\n",
    "checkpointLocation = f\"{work_dir}/tmp/{queryName}\"\n",
    "!rm -rf $checkpointLocation\n",
    "fileSchema = (\n",
    "    StructType()\n",
    "    .add(StructField(\"emp_id\", LongType()))\n",
    "    .add(StructField(\"emp_name\", StringType()))\n",
    ")\n",
    "fileReader = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"json\")\n",
    "    .schema(fileSchema)\n",
    "    .load(filePath)\n",
    ")\n",
    "fileSelector = fileReader.select(\"emp_id\", \"emp_name\")\n",
    "fileWriter = (\n",
    "    fileSelector\n",
    "    .writeStream\n",
    "    .queryName(queryName)\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"update\")\n",
    ")\n",
    "fileTrigger = (\n",
    "    fileWriter\n",
    "    .trigger(processingTime=\"1 second\")\n",
    "    .option(\"checkpointLocation\", checkpointLocation)\n",
    ")\n",
    "fileQuery = fileTrigger.start()\n",
    "displayStream(\"fileSource\", f\"select * from {queryName}\", 3, 3)\n",
    "fileQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Kafka 소스 스트리밍 실습\n",
    "\n",
    "![table.8-1](images/table.8-1.png)\n",
    "\n",
    "#### 1. 카프카에 데이터 쓰기 - producer.py\n",
    "```python\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "from time import sleep\n",
    "from json import dumps\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "import names\n",
    "\n",
    "# 카프카에 데이터를 전송하는 코드\n",
    "def produce(port):\n",
    "    try:\n",
    "        hostname=\"kafka:%d\" % port\n",
    "        producer = KafkaProducer(bootstrap_servers=[hostname],\n",
    "            value_serializer=lambda x: dumps(x).encode('utf-8')\n",
    "        )\n",
    "        data = {}\n",
    "        for seq in range(9999):\n",
    "            print(\"Sequence\", seq)\n",
    "            first_name = names.get_first_name()\n",
    "            last_name = names.get_last_name()\n",
    "            data[\"first\"] = first_name\n",
    "            data[\"last\"] = last_name\n",
    "            producer.send('events', value=data)\n",
    "            sleep(0.5)\n",
    "    except:\n",
    "        import traceback\n",
    "        traceback.print_exc(sys.stdout)\n",
    "\n",
    "# 카프카의 데이터 수신을 위한 내부 포트는 9093\n",
    "if __name__ == \"__main__\":\n",
    "    port = 9093\n",
    "    if len(sys.argv) == 2:\n",
    "        port = int(sys.argv[1])\n",
    "    produce(port)\n",
    "```\n",
    "#### 2. 터미널에 접속하여 카프카 메시지를 생성합니다\"\n",
    "```bash\n",
    "(base) cd /home/jovyan/work/lgde-spark-stream/python\n",
    "(base) python producer.py\n",
    "```\n",
    "\n",
    "#### 3. 스파크를 통해 생성된 스트리밍 데이터를 확인합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[kafkaSource] Iteration: 10, Query: select first, count from kafkaSource order by count desc limit 5'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>first</th><th>count</th></tr>\n",
       "<tr><td>Richard</td><td>2</td></tr>\n",
       "<tr><td>Mary</td><td>2</td></tr>\n",
       "<tr><td>Nicole</td><td>2</td></tr>\n",
       "<tr><td>Barbara</td><td>2</td></tr>\n",
       "<tr><td>Lori</td><td>2</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+-----+\n",
       "|  first|count|\n",
       "+-------+-----+\n",
       "|Richard|    2|\n",
       "|   Mary|    2|\n",
       "| Nicole|    2|\n",
       "|Barbara|    2|\n",
       "|   Lori|    2|\n",
       "+-------+-----+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 컨테이너 내부에서 토픽에 접근하기 위한 포트는 9093\n",
    "kafkaReader = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9093\")\n",
    "    .option(\"subscribe\", \"events\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "kafkaSchema = (\n",
    "    StructType()\n",
    "    .add(StructField(\"first\", StringType()))\n",
    "    .add(StructField(\"last\", StringType()))\n",
    ")\n",
    "\n",
    "kafkaSelector = (\n",
    "    kafkaReader\n",
    "    .select(\n",
    "        from_json(col(\"value\").cast(\"string\"), kafkaSchema).alias(\"name\")\n",
    "    )\n",
    "    .selectExpr(\"name.first as first\", \"name.last as last\")\n",
    "    .groupBy(\"first\")\n",
    "    .count().alias(\"count\")\n",
    ")\n",
    "\n",
    "queryName = \"kafkaSource\"\n",
    "kafkaWriter = (\n",
    "    kafkaSelector\n",
    "    .writeStream\n",
    "    .queryName(queryName)\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"complete\")\n",
    ")\n",
    "\n",
    "checkpointLocation = f\"{work_dir}/tmp/{queryName}\"\n",
    "!rm -rf $checkpointLocation\n",
    "kafkaTrigger = (\n",
    "    kafkaWriter\n",
    "    .trigger(processingTime=\"1 second\")\n",
    "    .option(\"checkpointLocation\", checkpointLocation)\n",
    ")\n",
    "kafkaQuery = kafkaTrigger.start()\n",
    "displayStream(\"kafkaSource\", f\"select first, count from {queryName} order by count desc limit 5\", 10, 3)\n",
    "kafkaQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaQuery.status\n",
    "kafkaQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Cassandra 싱크 스트리밍 실습\n",
    "\n",
    "> 빌트인으로 지원하지 않는 저장소 엔진(예: Cassandra 등)의 경우 foreachBatch() 혹은 foreach() 메소드를 통해 커스텀 로직을 추가할 수 있습니다.\n",
    "\n",
    "\n",
    "### 3.1 모든 스토리지 시스템에 쓰기\n",
    "\n",
    "> 현재 '스파크 스트리밍'에는 카산드라에 스트리밍 데이터프레임을 저장할 수 있는 방법은 없기 때문에, Batch DataFrame 방식으로 저장합니다\n",
    "\n",
    "* foreachBatch : 임의의 작업들 혹은 매 '마이크로 배치'의 결과에 대해 커스텀 로직을 적용할 수 있는 배치 메소드\n",
    "  - foreachBatch(arg1: DataFrame or Dataset, arg2: Unique Identifier)\n",
    "* 로컬 파일을 카산드라 데이터베이스에 저장합니다\n",
    "  - 로컬에 저장된 JSON 파일을 읽어서 스트리밍 처리를 합니다\n",
    "  - 초기 test_keyspace 와 test_table 에 저정합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cassandraAddress = \"cassandra\"\n",
    "cassandraKeyspace = \"test_keyspace\"\n",
    "cassandraTableName = \"test_table\"\n",
    "spark.conf.set(\"spark.cassandra.connection.host\", cassandraAddress)\n",
    "\n",
    "def writeCountsToCassandra(updatedCountsDF, batchId):\n",
    "    # Use Cassandra batch data source to write the updated counts\n",
    "    (\n",
    "        updatedCountsDF\n",
    "        .write\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\n",
    "        .mode(\"append\")\n",
    "        .options(table=cassandraTableName, keyspace=cassandraKeyspace)\n",
    "        .save()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/09 12:10:16 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 2008 milliseconds\n"
     ]
    }
   ],
   "source": [
    "cassandraSchema = (\n",
    "    StructType()\n",
    "    .add(StructField(\"emp_id\", IntegerType()))\n",
    "    .add(StructField(\"emp_name\", StringType()))\n",
    ")\n",
    "cassandraPath = f\"{work_data}/json\"\n",
    "cassandraReader = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"json\")\n",
    "    .schema(cassandraSchema)\n",
    "    .load(cassandraPath)\n",
    ")\n",
    "\n",
    "cassandraSelector = cassandraReader.select(\"emp_id\", \"emp_name\")\n",
    "\n",
    "queryName = \"cassandraSink\"\n",
    "checkpointLocation = f\"{work_dir}/tmp/{queryName}\"\n",
    "!rm -rf $checkpointLocation\n",
    "\n",
    "cassandraWriter = (\n",
    "    cassandraSelector\n",
    "    .writeStream\n",
    "    .foreachBatch(writeCountsToCassandra)\n",
    "    .outputMode(\"update\")\n",
    ")\n",
    "\n",
    "cassandraTrigger = (\n",
    "    cassandraWriter\n",
    "    .trigger(processingTime=\"1 second\")\n",
    "    .option(\"checkpointLocation\", checkpointLocation)\n",
    ")\n",
    "\n",
    "cassandraQuery = cassandraTrigger.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 카산드라에 접속하여, 데이터를 조회합니다\n",
    "\n",
    "```bash\n",
    "$> docker-compose exec cassandra cqlsh\n",
    "cqlsh> use test_keyspace;\n",
    "cqlsh:test_keyspace> select emp_id, emp_name from test_table;\n",
    "```\n",
    "\n",
    "* 아래와 같이 나오면 성공입니다\n",
    "\n",
    "```bash\n",
    "Connected to Test Cluster at 127.0.0.1:9042.\n",
    "[cqlsh 5.0.1 | Cassandra 3.11.10 | CQL spec 3.4.4 | Native protocol v4]\n",
    "Use HELP for help.\n",
    "cqlsh> use test_keyspace;\n",
    "cqlsh:test_keyspace> select emp_id, emp_name from test_table;\n",
    "\n",
    " emp_id | emp_name\n",
    "--------+------------------\n",
    "      1 |         엘지전자\n",
    "      2 |         엘지화학\n",
    "    100 |             엘지\n",
    "      3 | 엘지에너지솔루션\n",
    "\n",
    "(4 rows)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cassandraQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### foreachBatch() 로 아래의 동작들이 가능합니다\n",
    "\n",
    "* 이전에 존재하는 배치 데이터 소스들을 재사용 하기 - *Reuse existing batch data sources* \n",
    "  - 이전 예제와 같이 배치에서 사용하는 데이터 소스들을 그대로 사용할 수 있습니다\n",
    "\n",
    "* 다수의 경로에 저장하기 - *Write to multiple locations*\n",
    "  - OLAP 및 OLTP 데이터베이스 등에 다양한 위치로 저장할 수 있습니다\n",
    "  - 출력 데이터에 대해 데이터프레임 캐시를 통해 재계산을 회피할 수 있습니다\n",
    "\n",
    "```python\n",
    "# In Python\n",
    "def writeCountsToMultipleLocations(updatedCountsDF, batchId):\n",
    "    updatedCountsDF.persist()\n",
    "    updatedCountsDF.write.format(...).save() # Location 1\n",
    "    updatedCountsDF.write.format(...).save() # Location 2\n",
    "    updatedCountsDF.unpersist()\n",
    "}\n",
    "```\n",
    "\n",
    "* 추가적인 데이터프레임 연산 하기 - *Apply additional DataFrame operations*\n",
    "  - Incremental Plan 생성이 어렵기 때문에 '스트리밍 데이터프레임'에 대한 연산 지원이 되지 않습니다 - **아래**\n",
    "  - '마이크로 배치' 출력에 대하여 foreachBatch 메소드를 통해 작업 수행이 가능합니다.\n",
    "  - **at-least-once** 지원만 가능하며, **exactly-once** 는 후처리 dedup 작업을 통해 가능합니다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### '스파크 스트리밍'에 지원되지 않는 연산 - Unsupported Operations\n",
    "\n",
    "> 스트리밍 '데이터프레임/데이터셋' 에서는 몇 가지 지원하지 않는 연산이 있습니다. 실행 시에는 ***DataFrame/Datasets 에서 지원하지 않는 연산이라는 AnalysisException*** 을 반환합니다. 생각해보면 끝없이 수신되는 데이터에 대한 정렬을 하는 것은 모호한 동작이며, 수신된 데이터를 모두 모니터링 해야 하므로, 비효율적인 연산일 가능성이 높습니다.\n",
    "\n",
    "| 연산자 | 설명 |\n",
    "| --- | --- |\n",
    "| 다수 스트리밍 집계 연산 | 여러개의 스트리밍 데이터프레임의 집계는 현재 지원하지 않습니다 |\n",
    "| limit or take | 스트리밍 데이터셋은 일부 로우를 가져오는 연산은 지원하지 않습니다 |\n",
    "| distinct | 스트리밍 데이터셋은 유일 데이터 연산은 지원하지 않습니다 |\n",
    "| sort | 집계 연산 이후의 Complete 출력모드에서만 정렬을 사용할 수 있습니다 |\n",
    "| outer join | 스트림 사이드에 대해서만 left inner, outer 조인을 지원 |\n",
    "| count | 단일 빈도수를 반환하지 않기 때문에, 실행 횟수를 반환하는 ds.groupBy().count() 를 사용하세요 |\n",
    "| foreach | 대신 ds.writeStream.foreach(...) 를 사용하세요 (다음 챕터에서) |\n",
    "| show | 콘솔 sink 를 사용하세요 (다음 챕터에서) |\n",
    "| cube, rollup | 연산 비용 및 실용성 면에서 지원하지 않는 연산 UnsupportedOperationException |\n",
    "\n",
    "* [support-matrix-for-joins-in-streaming-queries](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#support-matrix-for-joins-in-streaming-queries)\n",
    "  - 양쪽이 모두 스트림인 경우는 워터마크를 사용해야만 합니다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. MySQL 싱크 스트리밍 실습\n",
    "\n",
    "### 4.1 foreach 메소드를 이용하여 화면에 직접 출력 하는 예제\n",
    "\n",
    "* 커스텀 저장 로직을 매 로우마다 적용하는 메소드 (고정된 map 함수 같은 방식)\n",
    "  - 배치저장을 위한 인터페이스가 제공되지 않는 경우 직접 데이터를 저장하는 로직을 구현해야 하며, open, process, close 단계에 해당하는 구현을 해야만 합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysqlSchema = (\n",
    "    StructType()\n",
    "    .add(StructField(\"emp_id\", IntegerType()))\n",
    "    .add(StructField(\"emp_name\", StringType()))\n",
    ")\n",
    "mysqlPath = f\"{work_data}/json\"\n",
    "mysqlReader = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"json\")\n",
    "    .schema(mysqlSchema)\n",
    "    .load(mysqlPath)\n",
    ")\n",
    "\n",
    "# 화면에 데이터를 그대로 출력하는 예제\n",
    "def process_row(row):\n",
    "    print(\"%d, %s\" % (row[0], row[1]))\n",
    "\n",
    "mysqlWriter = (\n",
    "    mysqlReader\n",
    "    .writeStream\n",
    "    .foreach(process_row)\n",
    ")\n",
    "\n",
    "queryName = \"mysqlSink\"\n",
    "checkpointLocation = f\"{work_dir}/tmp/{queryName}\"\n",
    "!rm -rf $checkpointLocation\n",
    "mysqlTrigger = (\n",
    "    mysqlWriter\n",
    "    .trigger(processingTime=\"1 second\")\n",
    "    .option(\"checkpointLocation\", checkpointLocation)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysqlQuery = mysqlTrigger.start()\n",
    "mysqlQuery.awaitTermination(5)\n",
    "mysqlQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 4.2 ForeachWriter 제 실습을 위하여 python 기반의 mysql connector 예제 (참고)\n",
    "\n",
    "> foreach 구문에서는 디버깅이 어렵기 때문에 온전하게 동작하는 mysql python 예제를 미리 작성합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import errorcode\n",
    "\n",
    "cnx = mysql.connector.connect(user='sqoop', password='sqoop', host='mysql', database='testdb')\n",
    "cursor = cnx.cursor()\n",
    "\n",
    "def createEmployee(cursor):\n",
    "    try:\n",
    "        employee = \"CREATE TABLE employees ( emp_id int, emp_name varchar(30) )\"\n",
    "        cursor.execute(employee)\n",
    "    except mysql.connector.Error as err:\n",
    "        if err.errno == errorcode.ER_TABLE_EXISTS_ERROR:\n",
    "            return \"employees table already exists.\"\n",
    "        else:\n",
    "            return err.msg\n",
    "    else:\n",
    "        return \"employees table created.\"\n",
    "\n",
    "print(createEmployee(cursor))\n",
    "\n",
    "def deleteEmployee(delete_employee):\n",
    "    try:\n",
    "        cursor.execute(delete_employee)\n",
    "    except:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return \"failed to delete employees\"\n",
    "    return \"employees table deleted.\"\n",
    "\n",
    "delete_employee = \"DELETE FROM employees\"\n",
    "print(deleteEmployee(delete_employee))\n",
    "\n",
    "def insertEmployee(add_employee, data_employee):\n",
    "    emp_no = -1\n",
    "    try:\n",
    "        cursor.execute(add_employee, data_employee)\n",
    "        emp_no = cursor.lastrowid\n",
    "    except:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return emp_no\n",
    "    return emp_no\n",
    "\n",
    "add_employee = \"INSERT INTO employees ( emp_id, emp_name ) VALUES ( %s, %s )\"\n",
    "data_employee = ( 1, '박수혁' )\n",
    "print(insertEmployee(add_employee, data_employee))\n",
    "\n",
    "cnx.commit()\n",
    "cursor.close()\n",
    "cnx.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 저장된 데이터를 확인합니다\n",
    "```bash\n",
    "$ docker-compose exec mysql mysql -usqoop -psqoop\n",
    "mysql> use testdb;\n",
    "mysql> select emp_id, emp_name from employees;\n",
    "+--------+--------------------------+\n",
    "| emp_id | emp_name                 |\n",
    "+--------+--------------------------+\n",
    "|      1 | 엘지전자                   |\n",
    "|      2 | 엘지화학                   |\n",
    "|      3 | 엘지에너지솔루션             |\n",
    "|    100 | 엘지                      |\n",
    "+--------+--------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4.3 ForeachWriter 클래스를 통해서 외부 데이터베이스에 저장하는 예제\n",
    "\n",
    "> 아래의 경우는 foreach 메소드를 사용하는 예제를 설명하기 위함이며, 실무에서는 ***사용하지 사용해서는 안 되는 예제*** 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import errorcode\n",
    "\n",
    "class ForeachWriter:    \n",
    "    \n",
    "    def createEmployee(self, cursor):\n",
    "        try:\n",
    "            employee = \"CREATE TABLE employees ( emp_id int, emp_name varchar(30) )\"\n",
    "            cursor.execute(employee)\n",
    "        except mysql.connector.Error as err:\n",
    "            if err.errno == errorcode.ER_TABLE_EXISTS_ERROR:\n",
    "                return \"employees table already exists.\"\n",
    "            else:\n",
    "                return err.msg\n",
    "        else:\n",
    "            return \"employees table created.\"\n",
    "        \n",
    "    def deleteEmployee(self, cursor):\n",
    "        try:\n",
    "            delete_employee = \"DELETE FROM employees\"\n",
    "            cursor.execute(delete_employee)\n",
    "        except:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return \"failed to delete employees\"\n",
    "        return \"employees table deleted.\"\n",
    "        \n",
    "    def insertEmployee(self, cursor, data_employee):\n",
    "        emp_no = -1\n",
    "        try:\n",
    "            add_employee = \"INSERT INTO employees ( emp_id, emp_name ) VALUES ( %s, %s )\"\n",
    "            cursor.execute(add_employee, data_employee)\n",
    "            emp_no = cursor.lastrowid\n",
    "        except:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return emp_no\n",
    "        return emp_no\n",
    "    \n",
    "    def insertDocument(self, data_employee):\n",
    "        emp_id = -1\n",
    "        try:\n",
    "            cnx = mysql.connector.connect(user='sqoop', password='sqoop', host='mysql', database='testdb')\n",
    "            cursor = cnx.cursor()\n",
    "            emp_id = self.insertEmployee(cursor, data_employee)\n",
    "            cnx.commit()\n",
    "        except:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            cnx.close()\n",
    "        return emp_id\n",
    "    \n",
    "    def selectEmployee(self, cursor):\n",
    "        emp_no = -1\n",
    "        try:\n",
    "            get_employees = \"SELECT emp_id, emp_name FROM employees\"\n",
    "            cursor.execute(get_employees)\n",
    "            for (emp_id, emp_name) in cursor:\n",
    "                print(\"{} in '{}' has retrieved.\".format(emp_id, emp_name))\n",
    "        except:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return emp_no\n",
    "        return emp_no\n",
    "    \n",
    "    # 데이터베이스 커넥션, 테이블 생성 및 테이블 데이터 삭제\n",
    "    def open(self, partitionId, epochId):\n",
    "        try:\n",
    "            cnx = mysql.connector.connect(user='sqoop', password='sqoop', host='mysql', database='testdb')\n",
    "            cursor = cnx.cursor()\n",
    "            print(self.createEmployee(cursor))\n",
    "            print(self.deleteEmployee(cursor))\n",
    "            cnx.commit()\n",
    "        except:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            cnx.close()\n",
    "        return True\n",
    "    \n",
    "    def process(self, row):\n",
    "        data_employee = (row[0], row[1])\n",
    "        result = self.insertDocument(data_employee)\n",
    "        if (result >= 0):\n",
    "            print(\"[%s] '%s' is inserted\" % data_employee)\n",
    "    \n",
    "    def close(self, error):\n",
    "        try:\n",
    "            cnx = mysql.connector.connect(user='sqoop', password='sqoop', host='mysql', database='testdb')\n",
    "            cursor = cnx.cursor()\n",
    "            self.selectEmployee(cursor)\n",
    "        except:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysqlSchema = (\n",
    "    StructType()\n",
    "    .add(StructField(\"emp_id\", IntegerType()))\n",
    "    .add(StructField(\"emp_name\", StringType()))\n",
    ")\n",
    "mysqlPath = f\"{work_data}/json\"\n",
    "mysqlReader = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"json\")\n",
    "    .schema(mysqlSchema)\n",
    "    .load(mysqlPath)\n",
    ")\n",
    "\n",
    "# ForeachWriter 객체를 생성하여 전달합니다\n",
    "mysqlWriter = (\n",
    "    mysqlReader\n",
    "    .writeStream\n",
    "    .foreach(ForeachWriter())\n",
    ")\n",
    "\n",
    "queryName = \"mysqlSink\"\n",
    "checkpointLocation = f\"{work_dir}/tmp/{queryName}\"\n",
    "!rm -rf $checkpointLocation\n",
    "mysqlTrigger = (\n",
    "    mysqlWriter\n",
    "    .trigger(processingTime=\"1 second\")\n",
    "    .option(\"checkpointLocation\", checkpointLocation)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "employees table created.\n",
      "employees table deleted.\n",
      "[1] 'OO전자' is inserted\n",
      "[2] 'OO화학' is inserted\n",
      "[3] 'OO에너지솔루션' is inserted\n",
      "[100] 'OO' is inserted\n",
      "1 in 'OO전자' has retrieved.\n",
      "2 in 'OO화학' has retrieved.\n",
      "3 in 'OO에너지솔루션' has retrieved.\n",
      "100 in 'OO' has retrieved.\n"
     ]
    }
   ],
   "source": [
    "mysqlQuery = mysqlTrigger.start()\n",
    "mysqlQuery.awaitTermination(5)\n",
    "mysqlQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
